{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Homework 6: Trees, Bagging, Random Forests, and Boosting\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2019**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader and Chris Tanner\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML, display\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- As much as possible, try and stick to the hints and functions we import at the top of the homework, as those are the ideas and tools the class supports and is aiming to teach. And if a problem specifies a particular library you're required to use that library, and possibly others from the import list.\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is excessively long because output was not suppressed or otherwise limited. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>Note:</b><span style = 'color:black'> Make sure your submission passes all assert statements we've provided in this notebook.</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Boson Discovery\n",
    "\n",
    "The discovery of the Higgs boson in July 2012 marked a fundamental breakthrough in particle physics. The Higgs boson particle was discovered through experiments at the Large Hadron Collider at CERN, by colliding beams of protons at high energy. A key challenge in analyzing the results of these experiments is to differentiate between collisions that produce Higgs bosons and collisions that produce only background noise. \n",
    "\n",
    "You are provided with data from Monte-Carlo simulations of collisions of particles in a particle collider experiment. The training set is available in `Higgs_train.csv` and the test set is in `Higgs_test.csv`. Each row in these files corresponds to a particle collision described by 28 features (columns 1-28), of which the first 21 features are kinematic properties measured by the particle detectors in the accelerator, and the remaining features are derived by physicists from the first 21 features. The class label is provided in the last column, with a label of 1 indicating that the collision produces Higgs bosons (signal), and a label of 0 indicating that the collision produces other particles (background).\n",
    "\n",
    "The data set provided to you is a small subset of the HIGGS data set in the UCI machine learning repository. The following paper contains further details about the data set and the predictors used: [Baldi et al., Nature Communications 5, 2014](https://www.nature.com/articles/ncomms5308)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 training samples, 5000 test samples\n",
      "\n",
      "Columns:\n",
      "lepton pT, lepton eta, lepton phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb, class\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_train = pd.read_csv('Higgs_train.csv')\n",
    "data_test = pd.read_csv('Higgs_test.csv')\n",
    "\n",
    "print(f\"{len(data_train)} training samples, {len(data_test)} test samples\")\n",
    "print(\"\\nColumns:\")\n",
    "print(', '.join(data_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>jet 2 eta</th>\n",
       "      <th>jet 2 phi</th>\n",
       "      <th>jet 2 b-tag</th>\n",
       "      <th>jet 3 pt</th>\n",
       "      <th>jet 3 eta</th>\n",
       "      <th>jet 3 phi</th>\n",
       "      <th>jet 3 b-tag</th>\n",
       "      <th>jet 4 pt</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.377</td>\n",
       "      <td>-1.5800</td>\n",
       "      <td>-1.7100</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.114</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.620</td>\n",
       "      <td>-1.480</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1.280</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.110</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.522</td>\n",
       "      <td>1.320</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.360</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.310</td>\n",
       "      <td>1.080</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.707</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>0.919</td>\n",
       "      <td>-1.230</td>\n",
       "      <td>1.170</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>0.886</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.300</td>\n",
       "      <td>0.7620</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.459</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.160</td>\n",
       "      <td>2.220</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.938</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.617</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>-1.3500</td>\n",
       "      <td>1.150</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.060</td>\n",
       "      <td>-0.0194</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.470</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.490</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.917</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.851</td>\n",
       "      <td>-0.3810</td>\n",
       "      <td>-0.0713</td>\n",
       "      <td>1.470</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.620</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>1.180</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.796</td>\n",
       "      <td>-1.520</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.987</td>\n",
       "      <td>1.350</td>\n",
       "      <td>1.460</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.768</td>\n",
       "      <td>-0.6920</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.874</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>1.320</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.502</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.983</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton pT  lepton eta  lepton phi  missing energy magnitude  missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag  jet 2 pt  jet 2 eta  jet 2 phi  jet 2 b-tag  jet 3 pt  jet 3 eta  jet 3 phi  jet 3 b-tag  jet 4 pt  jet 4 eta  jet 4 phi  jet 4 b-tag   m_jj  m_jjj   m_lv  m_jlv   m_bb  m_wbb  m_wwbb  class\n",
       "0      0.377     -1.5800     -1.7100                     0.991               0.114     1.250      0.620     -1.480         2.17     0.754     0.7750     -0.667         2.21     1.280     -1.190      0.505         0.00     1.110     -0.464      0.397         0.00  0.522  1.320  0.982  1.360  0.965  1.310   1.080    1.0\n",
       "1      0.707      0.0876     -0.4000                     0.919              -1.230     1.170     -0.553      0.886         2.17     1.300     0.7620     -1.060         2.21     0.607      0.459      1.020         0.00     0.497      0.956      0.236         0.00  0.440  0.829  0.992  1.160  2.220  1.190   0.938    1.0\n",
       "2      0.617      0.2660     -1.3500                     1.150               1.040     0.955      0.377     -0.148         0.00     1.060    -0.0194      1.110         0.00     1.470      0.205     -1.060         2.55     1.490     -0.398     -0.542         0.00  1.020  1.030  0.986  0.928  1.370  0.982   0.917    1.0\n",
       "3      0.851     -0.3810     -0.0713                     1.470              -0.795     0.692      0.883      0.497         0.00     1.620     0.1240      1.180         1.11     1.290      0.160     -0.916         2.55     0.945      0.796     -1.520         0.00  1.200  1.100  0.987  1.350  1.460  0.995   0.954    1.0\n",
       "4      0.768     -0.6920     -0.0402                     0.615               0.144     0.749      0.397     -0.874         0.00     1.150     0.1270      1.320         2.21     0.730     -0.758     -1.120         0.00     0.848      0.107      0.502         1.55  0.922  0.864  0.983  1.370  0.601  0.919   0.957    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>jet 2 eta</th>\n",
       "      <th>jet 2 phi</th>\n",
       "      <th>jet 2 b-tag</th>\n",
       "      <th>jet 3 pt</th>\n",
       "      <th>jet 3 eta</th>\n",
       "      <th>jet 3 phi</th>\n",
       "      <th>jet 3 b-tag</th>\n",
       "      <th>jet 4 pt</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.978645</td>\n",
       "      <td>-0.014280</td>\n",
       "      <td>-0.018956</td>\n",
       "      <td>1.005793</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.980390</td>\n",
       "      <td>0.025014</td>\n",
       "      <td>-0.007104</td>\n",
       "      <td>0.993678</td>\n",
       "      <td>0.988659</td>\n",
       "      <td>-0.010310</td>\n",
       "      <td>-0.006926</td>\n",
       "      <td>1.006922</td>\n",
       "      <td>0.997004</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>1.011994</td>\n",
       "      <td>0.982806</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>1.007810</td>\n",
       "      <td>1.038431</td>\n",
       "      <td>1.027201</td>\n",
       "      <td>1.054719</td>\n",
       "      <td>1.023094</td>\n",
       "      <td>0.958464</td>\n",
       "      <td>1.033432</td>\n",
       "      <td>0.960494</td>\n",
       "      <td>0.524600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.547025</td>\n",
       "      <td>1.011927</td>\n",
       "      <td>0.997945</td>\n",
       "      <td>0.591907</td>\n",
       "      <td>1.003337</td>\n",
       "      <td>0.463677</td>\n",
       "      <td>1.002018</td>\n",
       "      <td>1.014559</td>\n",
       "      <td>1.028920</td>\n",
       "      <td>0.476462</td>\n",
       "      <td>1.007983</td>\n",
       "      <td>1.002177</td>\n",
       "      <td>1.045206</td>\n",
       "      <td>0.471681</td>\n",
       "      <td>1.007824</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>1.200416</td>\n",
       "      <td>0.497681</td>\n",
       "      <td>1.007999</td>\n",
       "      <td>1.008904</td>\n",
       "      <td>1.400846</td>\n",
       "      <td>0.619460</td>\n",
       "      <td>0.353984</td>\n",
       "      <td>0.173243</td>\n",
       "      <td>0.427141</td>\n",
       "      <td>0.495720</td>\n",
       "      <td>0.352966</td>\n",
       "      <td>0.306057</td>\n",
       "      <td>0.499444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.275000</td>\n",
       "      <td>-2.410000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>-2.920000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>-2.910000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>-2.720000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.443000</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.587000</td>\n",
       "      <td>-0.764250</td>\n",
       "      <td>-0.877500</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>-0.870000</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>-0.659250</td>\n",
       "      <td>-0.885000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>-0.699000</td>\n",
       "      <td>-0.859500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664750</td>\n",
       "      <td>-0.679250</td>\n",
       "      <td>-0.858000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>-0.707250</td>\n",
       "      <td>-0.869250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.798750</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.772750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.846000</td>\n",
       "      <td>-0.009305</td>\n",
       "      <td>-0.016050</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>-0.023500</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>-0.004800</td>\n",
       "      <td>-0.030700</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>-0.004700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.877500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.725500</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>1.192500</td>\n",
       "      <td>0.692250</td>\n",
       "      <td>0.855500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>1.232500</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>1.140000</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.330000</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>6.260000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>4.190000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>2.730000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>5.770000</td>\n",
       "      <td>2.490000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>5.740000</td>\n",
       "      <td>3.940000</td>\n",
       "      <td>6.220000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>4.320000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lepton pT   lepton eta   lepton phi  missing energy magnitude  missing energy phi     jet 1 pt    jet 1 eta    jet 1 phi  jet 1 b-tag     jet 2 pt    jet 2 eta    jet 2 phi  jet 2 b-tag     jet 3 pt    jet 3 eta    jet 3 phi  jet 3 b-tag     jet 4 pt    jet 4 eta    jet 4 phi  jet 4 b-tag         m_jj        m_jjj         m_lv        m_jlv         m_bb        m_wbb       m_wwbb        class\n",
       "count  5000.000000  5000.000000  5000.000000               5000.000000         5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000\n",
       "mean      0.978645    -0.014280    -0.018956                  1.005793            0.002528     0.980390     0.025014    -0.007104     0.993678     0.988659    -0.010310    -0.006926     1.006922     0.997004     0.018817     0.003952     1.011994     0.982806     0.005201     0.003349     1.007810     1.038431     1.027201     1.054719     1.023094     0.958464     1.033432     0.960494     0.524600\n",
       "std       0.547025     1.011927     0.997945                  0.591907            1.003337     0.463677     1.002018     1.014559     1.028920     0.476462     1.007983     1.002177     1.045206     0.471681     1.007824     0.999656     1.200416     0.497681     1.007999     1.008904     1.400846     0.619460     0.353984     0.173243     0.427141     0.495720     0.352966     0.306057     0.499444\n",
       "min       0.275000    -2.410000    -1.740000                  0.010000           -1.740000     0.170000    -2.920000    -1.740000     0.000000     0.198000    -2.910000    -1.740000     0.000000     0.265000    -2.720000    -1.740000     0.000000     0.366000    -2.500000    -1.740000     0.000000     0.151000     0.443000     0.339000     0.371000     0.079500     0.413000     0.452000     0.000000\n",
       "25%       0.587000    -0.764250    -0.877500                  0.581000           -0.870000     0.676000    -0.659250    -0.885000     0.000000     0.666000    -0.699000    -0.859500     0.000000     0.664750    -0.679250    -0.858000     0.000000     0.619000    -0.707250    -0.869250     0.000000     0.798750     0.850000     0.986000     0.768000     0.672000     0.826000     0.772750     0.000000\n",
       "50%       0.846000    -0.009305    -0.016050                  0.903500            0.001300     0.891000     0.049500    -0.023500     1.090000     0.891000    -0.004800    -0.030700     1.110000     0.899500     0.045700     0.018800     0.000000     0.877000     0.012900    -0.004700     0.000000     0.898000     0.957000     0.990000     0.922000     0.868000     0.952000     0.877500     1.000000\n",
       "75%       1.220000     0.725500     0.837000                  1.300000            0.866000     1.160000     0.716000     0.894000     2.170000     1.192500     0.692250     0.855500     2.210000     1.232500     0.717000     0.855000     2.550000     1.220000     0.719000     0.859000     3.100000     1.030000     1.090000     1.030000     1.160000     1.120000     1.140000     1.060000     1.000000\n",
       "max       5.330000     2.430000     1.740000                  6.260000            1.740000     4.190000     2.960000     1.740000     2.170000     4.800000     2.910000     1.740000     2.210000     4.630000     2.730000     1.740000     2.550000     5.770000     2.490000     1.740000     3.100000    10.600000     5.740000     3.940000     6.220000     5.080000     4.320000     3.500000     1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_train.head())\n",
    "display(data_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into NumPy arrays\n",
    "X_train = data_train.iloc[:, data_train.columns != 'class'].values\n",
    "y_train = data_train['class'].values\n",
    "X_test = data_test.iloc[:, data_test.columns != 'class'].values\n",
    "y_test = data_test['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b>Question 1 [20pts]: A Single Model </b></div>\n",
    "We start by fitting a basic model we can compare the other models to. We will pick a decision tree as the base model because we will later include bagging, random forests and boosting and want a fair comparison. We will tune the decision tree using cross-validation (of course). We will be tuning the maximum tree depth; we refer to this parameter as \"depth\" for simplicity.\n",
    "\n",
    "Since we will only be using tree-based methods in this homework, we do not need to standardize or normalize the predictors.\n",
    "\n",
    "\n",
    "**1.1** Fit a decision tree model to the training set. Choose a range of tree depths from 1 to 20 and evaluate the  performance and standard deviations for each depth using 5-fold cross-validation. Plot the estimated mean +/- 2 standard deviations for each depth. Also, include the training set performance in your plot, but set the y-axis to focus on the cross-validation performance. \n",
    "Store the CV means and std variables `cvmeans`, `cvstds` and the train score `train_scores`  \n",
    "\n",
    "*Hint*: use `plt.fill_between` to shade the region.\n",
    "\n",
    "**1.2** Select an appropriate depth and justify your choice using your cross-validation estimates. Then report the classification accuracy on the **test set**. Store the training and test accuracies in variables named `best_cv_tree_train_score` and `best_cv_tree_test_score` to refer to in a later question.\n",
    "\n",
    "**1.3** What is the mechanism by which limiting the depth of the tree avoids over-fitting? What is one downside of limiting the tree depth? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Fit a decision tree model to the training set. Choose a range of tree depths from 1 to 20 and evaluate the  performance and standard deviations for each depth using 5-fold cross-validation. Plot the estimated mean +/- 2 standard deviations for each depth. Also, include the training set performance in your plot, but set the y-axis to focus on the cross-validation performance.\n",
    "Store the CV means and std variables `cvmeans`, `cvstds` and the train score `train_scores`\n",
    "\n",
    "*Hint*: use `plt.fill_between` to shade the region.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cs109Test(test_1.1a) ###\n",
    "\n",
    "depths = list(range(1, 21))\n",
    "cvmeans = []\n",
    "cvstds = []\n",
    "train_scores = []\n",
    "def calc_meanstd(X_train, y_train, depths):\n",
    "    for depth in depths:\n",
    "            dt = DecisionTreeClassifier(max_depth = depth)\n",
    "            cv_score = cross_val_score(dt, X_train,y_train,cv=5)\n",
    "            mean = np.mean(cv_score)\n",
    "            std = np.std(cv_score)\n",
    "            cvmeans.append(mean)\n",
    "            cvstds.append(std)\n",
    "            dt.fit(X_train,y_train)\n",
    "            train_scores.append(dt.score(X_train,y_train))\n",
    "    # your code here\n",
    "    # end of your code here\n",
    "    return cvmeans, cvstds, train_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvmeans, cvstds, train_scores = calc_meanstd(X_train,y_train,depths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d7f00dfb48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEJCAYAAACUk1DVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd3xUVdrA8d/0ZCa9kQKItAOIWJBiQQVdC7ZdRVTs7q661t19ra8NFLG3V11dV11dxYKoiKKuCiJFEYmC1EMPJKS3SZt67/vHncQEQjJAJpOE8/184tQ782QM95nTnmPSdR1FURRFMUc7AEVRFKVrUAlBURRFAVRCUBRFUUJUQlAURVEAlRAURVGUEGu0A9gfubm5DmAUUAgEoxyOoihKd2EBsoCfRo4c6d39wW6ZEDCSweJoB6EoitJNjQOW7H5nd00IhQCDBw/GbrdHO5Y9rFmzhuHDh0c7jDZ19RhVfAdGxXdgemp8Pp+PjRs3QugcurvumhCCAHa7HYfDEe1YWtVV42quq8eo4jswKr4D08Pja7WrXQ0qK4qiKIBKCIqiKEpId+0y2itN08jPz6euri5qMVitVtavXx+19w9HV49RxdeSy+Wid+/emM3qO5wSORFPCEKIBOB74Gwp5fbdHjsSeBVIABYB10spAwfyfmVlZZhMJoQQUfvHU1dXh8vlisp7h6urx6ji+42maRQUFFBWVkZGRkanvKdycIroGVMIMQZjatPgvTzlbeAmKeVgwAT8+UDfs6qqil69eqlvUkqPYTab6dWrF9XV1dEORenhIn3W/DNwI7Br9weEEIcAsVLKZaG73gAuPNA3DAaD2Gy2A30ZRelSbDYbgcABNZ6VbkDXdXQtaPwE/egBP5rfa/z4PGjeBjRfQ8TeP6JdRlLKPwEIIVp7OJuWc2ELgd4d8b4mk6kjXkZRugz1N905dF0HLQABL8F6N3rA33Ri1oN+9GDAuO73ovk96D4Pmt8bumzlduhErvs9oUsvms+LrgVAB3Qt9MbaPsVpP+xMGDmyw3//aA4qmzE+kkYmYJ8+lTVr1uxxn9VqjeqAcqPGGGpra3n++ef5+eefsVgsJCQk8Le//Y2hQ4dGOUL2+JyOPvpofv75Z2bPng3ApEmTWjw+d+5ccnNzmTZt2l5f86OPPsLpdHLGGWfw0ksvMWzYME466aQOiW9/LVmyhIceeoiRI0cyY8aMDnlN6Lj4wuXz+cjNzQ37+fvy3GjotPi0IJbaMizuIizuYiw1xZj9DaAFMWmB0GUQtIBxCSQDed/s29voABY7usVm/FjtYLGhW+zolhh0Z0Lotg1MFvTmSd5kAkyhy2a3MaGb2O0xM/70gRH5/KKZEPIxamo0yqSVrqW2DB8+fI/FGevXr4/6YGTjgKOmafzpT39izJgxzJ07F6vVyrJly7jllluYN28eycnJUY9xdy6XiyuvvLLVYxwOB1artc3Pd+3atYwePRqXy8Vtt93W4fHtj4ULF3LzzTdz0UUXdcjrQXQGve12O0cccURYz83NzWVkBL5BdpRIxaf5vfhKduAr2oq3eBu+oq34SnagB/0AmOwxOHodisWVg8liA4sNk9WKyWLDZLVhshjXC4qK6XPIocZtqw2T1d70mMlqw2RzYLY5MNljMNtiMNljjOd0Uktufz8/r9fb6hfpRlFLCFLKPCGERwhxvJRyKXA58EW04omEH3/8kcLCQm655ZamQe6xY8fyyCOPoGkaP/74I0888QSapjFo0CCmTp3Kvffei5QSk8nEH//4R37/+9+zYcMG7r//fgKBAA6Hg0ceeYScnBz+93//l02bNgEwZcoUJk+e3PTelZWVnH322SxcuBCbzcbGjRu57bbbmDt3Ls888wxLliyhtraWjIwMnnnmGdLS0pqOff755wG4+eabmTNnDi+99BJxcXHk5OTgdDoB+OKLL/j3v/+Nx+PB5/MxY8YMPB4PCxYsYNmyZaSnpzNv3jxGjx7N+eefz4cffsi///1vTCYThx12GPfddx8ul4sTTjiB008/ndzcXCwWC88++yx9+vRp8TlOmDCBM844g++//x6AGTNmMGzYMPLy8pg6dSpVVVXExMRw3333MWzYMO666y6qqqrIy8tjypQpzJ8/nx9++AGz2cwxxxzD/fffT1VVFU6nk3vuuYcRI0a0OOb2229n+vTpnHXWWSxduhSr1coNN9zA66+/Tl5eHnfeeScnnXQSGzdu5KGHHqK+vp6KigquvfZaLrnkEp5//nmKi4vJy8ujoKCACy+8kL/85S94vV6mTZtGbm4uNpuNG264gYkTJ/Lrr7/yyCOP4PF4SE5OZtq0aXt8BsqeNG+DcdIv3oa3aKtx8i/Nb+p+McfE4cg8lIRRE3Fk9see2R9bSiYmU/tDp1tzc0nswgk1Ujo9IQghPgful1KuAC4F/hWamvoz8H8d+V4LVuzg6+U7OvIlm/xudF8mHNO3zeesW7eOIUOG7DHjqbELZfPmzWzfvp1vv/2W+Ph4Hn/8cZKTk/nss8+oqKjgwgsvZMiQIbz55ptcffXVnHnmmXz88cesXLmSkpISqqurmTNnDsXFxTz11FMtEkJycjIjRoxgyZIljB8/nnnz5nHuueeSl5fH1q1beeONN4iPj+eOO+5g7ty5XHPNNXvEX1xczJNPPsmcOXNISkriuuuuw+l0omka7733Hi+//DIpKSnMnj2bV155hZdffpkJEyYwevRoxo0bx7x58wCQUvLyyy8za9asphPeCy+8wJ133klpaSnHHnss9913H48++igzZ87krrvu2iMWp9PJnDlzWLBgAXfeeSeffvopd955J/fffz/Dhg1j8+bN3Hjjjfz3v/8FICkpiZdffhn4rdVy/vnnM2nSJK699lpOO+00Vq5cya233trqMdOnTyctLY2PPvqIu+++m1deeYX//Oc//Pzzz8yYMYOTTjqJDz74gBtuuIFjjz2WnTt3cu6553LJJZc0/c4zZ86kpqaGU089lUsvvZRZs2ZRX1/PF198QXl5OVdddRWnnnoq9957Ly+//DLZ2dksXryY++67jzfeeKPdv8GDia4F8ZXuxLNzA978DXgLt+CvKKSx19niSsKe2Z+kQaOaTv7WxHQ19rKPOiUhSCn7Nbs+sdn1VcDozoghGsxmc7v1Rg499FDi4+MBWLZsWVMfd0pKCqeccgrLly/npJNO4sEHH2Tx4sVMmDCB8ePH43a72bZtG3/84x858cQTueOOO/Z47XPPPZd58+Yxfvx4vvjiC9566y169erFnXfeyccff8yuXbtYuXIlffu2nth++eUXjjrqqKbWwznnnMOyZcswm828+OKLLFiwgG3btrF8+fI2p/n+9NNPjB8/vqmL7KKLLuLuu+9uenzcuHEADBo0iBUrVrT6Go3JbsKECdx1110UFRWxZs2aFq9TX19PZWUlACNGjNjjNerq6tixYwennXYaAEceeSSJiYls3bq11WNOPPFEALKzs8nIyMBqtZKdnY3b7QbgrrvuYvHixfzzn/9k48aN1NfXNx07ZswY7HY7qampJCUlUVNTw08//cTkyZMxm81NLaiNGzeyc+dO/vKXvzQdW1tbu9fP8mCh+b14d23Gs3M9np0b8BRIdK/x+VriknFkDyJu+Im/nfzjo9f92pP0uJXKzU04pv1v8ZE0fPhw3nnnHXRdb/FN5emnn+a4447DZDIRExPTdL+u6y2O13WdYDDIGWecwVFHHcW3337LG2+8wcKFC5k+fTrz5s1j6dKlfPfdd/zhD39g3rx5JCQkNB1/yimn8Oijj/LTTz+RlZVFr169WLNmDf/zP//DlClTOP300zGbzXu8byOTydTiMavV+HOpq6tj0qRJnHvuuYwaNQohBDNnztzr56BpLecK6LreYgplY9Lc/f2aa3zvxtcLBoPY7XY++eSTpvuLiopISkoCaPG5Nn/f1u4LBoOtHtN8+nLz92/017/+lYSEBMaPH8/EiRP57LPP9vidmv9eVqu1xd9BXl4emqbRu3fvpt8jGAxSVlbW6mfQkwXr3caJP99IAN7CrcZsH8CW3oe4YccT02cIMX2GYk3MUN/8I0St3oqgY445htTUVF544YWmk87ixYv56KOPGDhw4B7PHzt2bNMMn4qKCubPn8/o0aP561//yurVq7n44ou59dZbWbduHfPnz+f222/n5JNP5t5778XpdFJY2LKird1uZ9y4ccyYMYNzzz0XML6tjx49mkmTJtGvXz8WLlzYFNvuRo4cycqVKykuLkbTND7//HMAtm/fjslk4vrrr2fMmDF8/fXXTa9hsVj2eL3Ro0ezYMECqqqqAJg1axZjxozZp8+ysfvp66+/ZsCAAeTk5NCvX7+mE+nSpUu59NJL23yNuLg4evfuzVdffQXAypUrKSsrY9CgQfsUS6OlS5dyyy23cOqpp7Jo0SKAvX6WAKNGjeLzzz9H13XKy8u57LLLyMnJobq6uqll9OGHHx7QYHx3oOs65roKalYtoPSzf7Dz5VvIe+Zqimc/RvVPn4PJTOKYs+k1+W4O+fsb9Ln2WdInXk/84SdjS+qlkkEE9egWQrSZTCb+8Y9/8Mgjj3D22WdjtVpJTk7mlVdeIS0tjS1btrR4/o033sjUqVM555xzCAaDXH/99Rx22GFcf/313HPPPbz44ovYbDamTp3K0KFD+eqrrzjrrLNwOByce+65ra73OO+885g7dy6nn346ABMnTuSmm25q6roYPnw4+fn5rcaflpbGvffey1VXXUVsbGxTEhsyZAhDhw7lzDPPxGQyccIJJzRNgTvuuON4+umnm7rBGp9/3XXXcfnll+P3+znssMPanLramsbpsLGxsTz66KMAPPHEE0ydOpVXX30Vm83GM8880+7JovGY559/HpvNxvPPP7/fe2rcfPPNTJkyBYfDwZAhQ8jJydnrZwnGwP/06dObkvN9991HfHw8zz33HA8//DBer5e4uDgee+yx/Yqnq/NXFlGzagE1vy4ksaacUoyB35jegvgRJxPTZyj2rAGYrV1vj5ODhWlvTfSuLDc3tx+wbW/TTqM9x7+r1+GBrh9j8/gmTJjAf/7zH3r37pB1ix0iGp/fvvxtd5Vpp1rAR738kZqV82nYvhpMZmL7H0GZoxfihNOxpfUOa9ZPZ+sqn9/edMC000NHjhy5fffHVQtBUZQO5y3eTs2q+dSuXoTmqcWamEHySZcQP2I81oRUduXmYk+P3vie0jqVEJQub8GCBdEOQQmD5q2ndu0SalbOx1u4GSxWXGIM8UeeQmy/w7tkS0BpSSUERVH2m67rePMl7pXzqVu/FN3vxZbel9TfXU3c8JOwOOPbfxGly1AJQVGUfRasq6Zm9XfUrPwGf3kBJnsMcYeNI/7IU3FkD1QzgboplRAURQmbt2gbVd9/RJ1cDloAR29B+tk34hp6LGZ7bLTDUw6QSgiKorTLW7ydysWzqJc/YnY4SRx1JvFHnII9XdVc6klUQlAUZa92TwTJ4y4iYfRZWGK67pRlZf+phBBB06ZN4+eff8bv97Njxw4GDBgAwBVXXMEFF1wQ1ms899xzDB8+nFNOOWWvzznvvPNalHCIlrvvvpvly5fzt7/9jbPPPjva4SgHoHkiMDmcJI2bTOLos1Ui6OFUQoigBx54AID8/HyuuOKK/Tpp33rrre0+pyskA4CPP/6YX3/9db9X/irR5y3eTtWSD6jbsMxIBCdcaCSC2Lhoh6Z0gh6dEGp+XUjNqsjMYY8/YgLxI07e7+Off/55Vq5cSWFhIZdddhkDBw7kmWeewePx4Ha7ufvuuzn11FO56667GD16NKNHj+amm25i0KBBrF+/ntTUVJ577jmSkpIQQiCl3Gsdfr/fzwMPPEBubi69ehm1YK655poWO5n9+OOP/OMf/8BqtZKfn8+IESN4+OGHsdvtzJkzhzfffBNN0zjssMN44IEHcDgcjB07luHDh1NaWkp6ejq6rnPhhRfy+uuvs3Dhwlb3P2h+zB133NFUdiI/P58JEybgdDr55ptvCAaDvPbaa6SlpfH222/zySef0NDQgM1m46mnnqJ///5MmDCBc889lyVLltDQ0MBjjz3G8OHDWb9+Pffffz8ej4fExESefPJJMjMzeeWVV/jiiy8IBoOccMIJ3H777Wo2TIivJI/KxR9Qt+EHlQgOYmqlSBT5fD4+//xzpkyZwttvv8306dP5+OOPmT59Os8999wez9+wYQNXX301n332GQkJCXz66ad7PEdKyWuvvcYHH3zAK6+8gtvt5r333qOhoYEvv/ySRx55hNWrV7cazy+//MI999zDl19+idfrZebMmWzatIlZs2bx3nvv8cknn5Camsprr70GGJvw/PnPf+aTTz7h1VdfBYzWSllZGS+//DJvvfUWn376KbGxsbzwwgt7HGO1Wlm1ahXTpk3jww8/ZObMmaSkpPDRRx8xaNAg5s2bR21tLd988w1vvfUWn332GSeffHKLyqpJSUnMnj2biy++mH/+858A3Hbbbdxwww18+umnTJw4kTfffJNFixaxZs0aZs+e3bSHxNy5cw/sf2AP4CvJo/jDJ8n/19+p37qSpBMm0ffGf5By0sUqGRyEenQLIX7EyQf0LT7Smtfff+KJJ/j222/58ssvWbVqVav79aampjJs2DDA2Dugurp6j+e0Vod/6dKlTJ48GZPJRE5ODscee2yr8YwaNYr+/fsDxrjErFmzsNls5OXlNe1H4Pf7m2IAWt3Ssb39D5ofM3jwYLKyjJ1Uk5OTm2LLysrC7XYTFxfHU089xbx589i+fTuLFy9uUc+n+V4KX331FRUVFZSWljJ+/HjAKCgH8Nhjj/Hrr79y/vnnA+DxeMjOzm71czgY+Ep2ULlkFnXrf8BkjyXphEmhFoFaSHYw69EJoatrXn9/ypQpjBkzhjFjxnDssce2WgK5tRr74TzHYrHssSdBaywWS9P1xuOCwSBnnnkm9957L2AUdWte4rm1fQfa2/+g+THN9xzYPQaAwsJCLr/8ci677DJOPPFE0tLSWL9+/R6/b2PXj81ma9EN5PV6KSkpIRgMcuWVV3L11VcD4Ha793ivg0GgppLyb/5N3bqlRiI4fhKJY1QiUAxhdRkJIWKFEIcLIUxCCGekgzrYVFVVsX37dm699VZOPPFE5s+f32Zd/X113HHHNdXhLy4uZvny5a32nefm5jbtfTBnzhxOPPHEpv0OysvL0XWdqVOn8uabb7b5fh2x/0Gj1atXc8ghh3DVVVdx+OGHN40v7E18fDy9evViyZIlgNGF9dxzzzF27Fg++eQT6urqCAQCLbbbPBjouk7N6oXkv3Ir9Rt/Ium48+l740uknHyJSgZKk3ZbCEKIscBHQAA4DlglhDhHSvl9pIM7WCQlJTFp0iTOOussrFYrY8eOxePxtNiS8UBMnjyZDRs2cM4555Cenk52dnarW3tmZGRwxx13UFxczPHHH8+FF16IxWLhpptu4sorr0TTNIYOHcq1117b5vt1xP4HjY4//njeffddJk6ciK7rjBo1ik2bNrV5TOOeB0888QTJyck8/vjjZGRksGHDBiZPnkwwGGTcuHH84Q9/2K+YuptATSVlX/yT+k0/4cgRpJ9zI/bUnGiHpXRFuq63+TN48ODFgwcPHjZ48OBfQrcnDh48+Kf2jovkz4oVK/qtWLFC93g8+u7WrVu3x32drba2NtohtPDtt9/qCxYs0HVd191utz5hwgS9oKCgxXOWLVumX3bZZdEIr1Vd7TPcXTTi25e/7RUrVuiapunu1d/p2568Qt/66MV65Q+f6FowEMEIw7dixYpoh9Cmnhqfx+PRV6xYoa9YsaKf3sq5NZwxBKeUcl3jblxSys+FEA9HNk0pHWnAgAHccccdPPvsswDccsstJCYmRjkqJZJM3lqKP3hMtQqUfRJOQvALIZIBHUC0tk+j0qX16dOHd999t8V9u89iahzQVro3XdepXbuYhCWv0KAFSTnlChJHn43JfPANoCv7LpyEMB34DsgUQrwLnAa03YkcZbquqwVHSo+ih7HVbaC2krIvXqF+43K0xBz6XnwH9rSus+2o0vW1mxCklJ8JITYAvwMswDQp5YaIR7afLBYLfr9flU9QehS/34/V2vo/V13XqVu3hLL/voru85JyyhVssWarZKDss3annQohegN/l1K+BHwNPCqEyIx4ZPspKSmpaeqkovQEmqZRXFzc6rhPoLaK4tmPUzLnWWzJWeT86UmSxp4HartKZT+E02X0BtC4xj8PWAi8DkyMTEgHJi0tjfz8fKSUUYvB5/N1+RZKV49RxdeSy+UiLS2t6fYerYIJl5M45hw1VqAckHASQpqU8v8ApJQe4FkhxJWRDWv/mc1m+vbtG9UYcnNzWy3p0JV09RhVfHsXqK2i7MtXqJc/4sgeRPo5N6nuIaVDhJMQrEKIbCnlLgAhRC9AjdgqShR4CjZSNOsRdG+DahUoHS6chPA0sFII8SXG1NNTgdsjGpWiKHuo3/ILxR8+gcWVROZlD6rtK5UO1+7Ik5TydYwZRr8AK4DTpZTvRDowRVF+U7t2MUWzHsGWnEX2lQ+rZKBERLhTEaowBpOXAnYhxNERi0hRlBaqf5pHyZxniek9hOzLH8QalxztkJQeKpzidg8CtwHFze7Wgf6RCkpRFGMmUeV371K19EOcg0eT8Ye/YbZ23ZlXSvcXzhjC5cDAxkFlRVEiT9eClH3xCjUrvyH+yFNJO/NaNXisRFw4CWGnSgaK0nm0gI+SOc9SL38k6fgLSD7pElWKRekU4SSE+UKIx4FPgIbGO6WUP0csKkU5SGmeOoo+eAzPjrWk/u5qEkefHe2QlINIOAnhqtDlhc3uU2MIitLBArVVFL03HV/pDjLO+ytxw8dFOyTlIBNOcbtD9/fFhRBTgHsBG/CslPLF3R4/E3gsdHM1cJ2UsnZ/309Ruit/ZRGF7z5EsLaSzMl34xxwVLRDUg5C4cwySsMYWI7DWKFswRhkvrSd43KAh4GRgBf4XgjxrZRyXejxJOBN4OTQBjx3ADOAWw7g91GUbsdbtI2i96ajawGyLp1KTM7gaIekHKTCWYcwC2N18h+BPsCVQDilRE8FFkgpK6SUdcBsYFKzxwcBeY0JAvgM+H24gStKT9CQt5Zdb98PZgvZVzyskoESVeGMIRwipRwghPgH8E9gKjAnjOOygcJmtwuB0c1ubwL6CCGOkFKuAiYD+1RWe82aNfvy9E6Vm5sb7RDa1dVj7Onx2YolrlVz0GKTqDn6EsryiiGvuP0Dw9TTP79IOxjjCychFIUuNwHDpZQzhRC2MI4zE9p2M8REs5aFlLJKCHEF8IoQwgz8C/CFF7Zh+PDhOByOfTmkU+Tm5jJy5Mhoh9Gmrh5jT4/PvfIbylZ+jCNrAJkX3YPFGd+B0fX8zy/Semp8Xq+3zS/S4SSEEiHE7cAPwDQhhBtwhnFcPtB8mkQm0LSeQQhhAfKllGNCt0cBW8J4XUXp1qq+/4iKb2cS2/9Iel1wO2Z7TLRDUhQgvDGE6wCvlHIJRnG7B4E7wzjuG+AUIUS6EMIJXAB82exxHfhKCJEjhDABfwfe36foFaWbqVj0PhXfziTusHFkTr5LJQOlSwln2mkJ0LhBzp2ElwyQUhYIIe4BvgXswKtSyuVCiM+B+6WUK4QQ12EkCQdGAnli/34NRen6Kha9T9XiWcSNmED62X/BpLa5VLqYcKadXgRMA1Ka3y+lzGjv2FCZ7Hd2u29is+vzgHnhBqso3ZVKBkp3EM4YwuPAzaj+fUXZL5WLZqlkoHQL4SSE7VLKuRGPRFF6oMpFs6hc/D5xI8arZKB0eeEkhDeFEE9g9PX7G++UUi6KWFSK0gNULm6WDM5SyUDp+sJJCOOBicAZze7TgRERiUhReoDKxbOoXPQ+cSNONpKB2stA6QbCSQhHAzlSSk+kg1GUnqBy8QfNksENKhko3UY4bdgiwkscinLQM5LBe8QdrpKB0v2Ec6IvAFYJIb7BqFoKgJRSVSVVlGYql8wOJYOTSD9bJQOl+wknIWxBTTlVlDZVLplN5XfvhpLBjSoZKN1SOAlhgJTyiohHoijdlEoGSk8RzhjCEaFaQ4qi7KYpGQw/USUDpdsLp4VQCKwVQiwDmra3VGMIysGucumHvyWDc25SyUDp9sJJCD+EfhRFCYnZspTKTd+pZKD0KOFUO50mhIjD2BvZBvwopayJeGSK0kVVLv2IWJUMlB6o3TGE0MY1G4FngaeBPCHEcZEOTFG6oqplc6lcOBNv1mEqGSg9TjhdRk8Bl0opvwUQQkzASAxjIxmYonQ17tz/UjH/TVxDj6Wyz0kqGSg9TjizjOIbkwGAlHIB4W2hqSg9Rs2vCyn78hWcA0eScd6tYFaF6pSeJ5y/al0IcUjjDSFEPyAYsYgUpYupXf8DpZ+9SEy/w8m44DZMFlu0Q1KUiAiny+hBYFmodIUOnA7cENGoFKWLqN+US8mcZ3DkDCLzwjsxW+3RDklRImavLQQhxO9DV7/AKIH9PbAcOFlK+WEnxKYoUdWwfTXFHz6BPaMfWRfdg9keG+2QFCWi2mohPATMAX6QUh4NbOickBQl+jz5Gyia9SjWlCyyLrkPc4wr2iEpSsS1lRDcQoiNQG8hxK+7PyilVBvkKD2St3ALhe89jDU+mawp92Nxxkc7JEXpFG0lhDOAo4DXgJs7JxxFiS5fyQ4K330QS4yLrEunYo1LjnZIitJp9poQQquRFwkhlkspv+vEmBQlKvwVuyh8Zxomi91IBglp0Q5JUTpVONNOh6tqp0pP568uYdfMaei6RtalD2BLzox2SIrS6VS1U+WgF6ipoHDmNHRfA1mXTsOe1jvaISlKVKhqp8pBLVhXTeE70wjWVZE15QEcmYdGOyRFiZpwq53GAgOBtUCMlLI+4pEpSoQFPXUUvvsQgaoSMi++l5icwdEOSVGiKpxqp2Mw9lSeB2QDO1W1U6W707wNFL03HV/pTnpNuoPYQw6LdkiKEnXhDCo/CZwKlEsp84HLgeciGpWiRJDm91L0wSN4d22m1/l/xzngqGiHpChdQkeFDRIAACAASURBVDgJwSmlXNd4Q0r5OeGNPShKl6MH/RR/+ASevHVknHcLLjEm2iEpSpcRTkLwCyGSMQrbIYQQkQ1JUSJD14KUzH2ehi2/kDbxeuIOGxftkBSlSwnnm/504DsgSwjxLnAacG1Eo1KUDqbrOuVfvU7duqWknHIFCUedGu2QFKXLCWeW0WdCiA3A7wAL8KCUcn3EI1OUDlS56H3cuV+SeOzvSRp7XrTDUZQuKdxtnxxATOj5gciFoygdr/qneVQt+YD4I04hZfxl0Q5HUbqscKadXg98CxwJjAaWCCEmRzowRekINWsWUf7V6zjFGNImXofJpKqwKMrehDOG8HfgKCllAYAQoi/wOTCrvQOFEFOAewEb8KyU8sXdHj8a+CdgB3YCl0kpq/bpN1CUvajflEvppy8Qc8hwMn7/V0xmS7RDUpQuLZwuo+rGZAAgpdwBeNo7SAiRAzwMnIDRurhWCDFst6c9B9wvpTwCkMBt4QauKG3x7FxP8UdPYs/op7a+VJQwhdNC+FoI8RLwIsb4wRXAptC3e6SUP+/luFOBBVLKCgAhxGxgEsYezY0sQELouhOo2OffQFF24y3eTtH7M7AmpJF18T2YHc5oh6Qo3UI4CeGS0OUZu93/IcbahP57OS4bo1Jqo0KMMYjm/g58JYR4FqgD1Coh5YD4K4soevchTPZYY7czV2K0Q1KUbsOk63pEXlgIcQ9GIbz7Qrf/DIyUUl4fuh0LrACullIuF0L8HThFSnlWe6+dm5vbD9gWkcCVbsvkrSV+2X8wBbzUjLkcLU5tcKMoe3HoyJEjt+9+ZyRLUOQDzZeCZgK7mt0eDjRIKZeHbv8TeGhf3mD48OE4HI4DCjIScnNzGTlyZLTDaFNXj3Ff4wt66ih86z78AQ9Zlz4Q8cqlPe3z62wqvgOzv/F5vV7WrFmz18fDXYewP74BThFCpAshnMAFwJfNHt8M9GlWCuM84KcIxqP0UJrfS/GsR/CVFdBr0h2qjLWi7KeIJYTQzKR7MNYwrATeCXUNfS6EOEZKWQlcBcwSQvwKXANcHal4lJ5JDwYo+egpPDs3kPH7W3H2PyLaISlKtxVWl5EQ4hAgBWha1dPG7CKaPecd4J3d7pvY7PoXwBfhBqsozem6RulnL1K/OZe0M68jbqjapkNRDkS7CUEI8SDG+oASQhVPaXt2kaJEnK7rlH/9b2rXLCL5pEtIOPq0aIekKN1eOC2Ey4GBUspd7T5TUTpJ1dIPcf/0OQmjzybp+AuiHY6i9AjhjCHsVMlA6UrcuV9S+d27xB1+EqmnXqnqEylKBwmnhTBfCPE48AnQ0HhnOGMIitLRatcspuzLV3EOHEn6WTdgMkVyopyiHFzCSQhXhS4vbHafGkNQOl3dhh8pmft/xBwyjIzz/weTRe3kqigdKZwNcg7tjEAUpS31W36h+OOncWQPJPPCuzHbut6CREXp7sKZZeQCngDOxChj/RXwVymlO8KxKQoADXlrKZ79OPb0PmRefC9mR2y0Q1KUHimcDthnMHZM+wPGamIdeD6SQSlKI0/BRopmzcCalEHWJfdhiXFFOyRF6bHC6YQdE9qvAGgqUrc2ciEpisFbtI2i96ZjcSWRNeUBVblUUSIsnBaCVQjR/HlmIBiheBQFAHNtGYXvPmiUsb70AazxKdEOSVF6vLCmnQLvCyFexugu+gtGfSJFiQh/ZRHxP72DyWol+9IHsCVmRDskRTkohNNC+DuwDpgBPI6x1eXtkQxKOXgF3GUUzpwKWpCsKQ9gS8mOdkiKctAIZ9ppAHgg9KMoEROoraRw5lSCnjpqj7kYe0bfaIekKAeVvSYEIcQSKeUJQogafitq10RKmdDKYYqyX4L1NRS+8yCBmgqyLrmfipK6aIekKAedtloIjSuTh7fymCoeo3QYzVNH4bsPEagoJPOi/yWmzxAoyY12WIpy0NlrQpBSFoauviylPLP5Y0KIZcDYSAamHBw0n4fC92fgK9lO5qQ7iT10RLRDUpSDVltdRrOBwcCA0I5mjWyAN9KBKT2fFvBRPPsxvAUbyfjD33AO6rp72CrKwaCtLqPbgH7Av4Cbm90fwJh1pCj7TQ8GKPnwSRq2/Ur6OTer3c4UpQtoq8toO7BdCCGklFrzx0L1jRRlv+hakJJPnjO2vjzjz8SPODnaISmKQngL084JbaMZhzGYbMHYXzk+koEpPZOuBSmd9xJ1678n5ZQrSRh5RrRDUhQlJJyE8CRwL3A98BhGkTtV6VTZZ5rfa7QM5I8kn3gRSWPPjXZIiqI0E85K5Top5fvAMsCDUbri7IhGpfQ4wbpqCt9+gHq5nNTfXU3yuMnRDklRlN2EkxA8QggHsBk4MjSesMdCNUXZG195AQVv3IWvJI9ek24ncbT6PqEoXVE4XUZzgXnAlcAPQohxQFlEo1J6jIYdayn+4HFMFgtZlz1ITM6gaIekKMpetNtCkFLOAK6RUhZgbJCzCJgU6cCU7q92zWIK33kQiyuR7KseUclAUbq4thamnbjb7X6hq78AQ4CSyIWldGe6rlP1/UdULnyHmL6H0WvSHVhi46IdlqIo7Wiry+jF0KUTOARjl7QAcDjGwrQjIxua0h3pwQBlX/6LmpXfEDf8RNLPugGT1RbtsBRFCUNbC9MOBxBCvA9cLqX8PnT7aOCezglP6U40bz3FHz1Jw9ZVJB0/ieSTLsZkUnUQFaW7CGeWkWhMBgBSyp+BgZELSemOAu5ydv3nHhq2ryHtrBtIOfkSlQwUpZsJZ5ZRgxDiKuAtjJXKfwKqIhmU0r14i7ZR9P4MNL+HzIvuwdn/iGiHpCjKfggnIfwReBt4FWP9QS4wJZJBKd1H/ZZfKP7oScwOFzlXTMeecUi0Q1IUZT+Fs4XmOuBoIURK6HZFxKNSugX3z19R9uW/sGccQuZF/4s1PiXaISmKcgDamnb6rJTyr0KIT2m2MlkIAYCUUhWiOUjpukblwneo+v5jYgccTa8//B2zIzbaYSmKcoDaaiHMD13O7oxAlO5BC/go/fQF6tYtJf7o00g7/U+YzJZoh6UoSgdoKyEsDXUTfdpZwShdW7CumqIPHsNbIEmZcDmJY89TM4kUpQdpKyGU8VtXUeO/ej10XcfYF0E5SPhKdlA06xGCdVVkXHAbcUOOjXZIiqJ0sLYWpoWzRqFNQogpGHsp2IBnpZQvNnvsSOCNZk9PByqllMMP9H2VjmXMJHoKs81B9uUP4chWy1AUpSdqd5aREMIOnEXLHdMGSinbXK0shMgBHgZGAl7geyHEt6FZS0gpVxIqfyGEcALLMTbhUbqQ6p8+p/zrfxsziSbfhTUhLdohKYoSIeGsQ3gf6A9kYRS2GwMsDOO4U4EFjdNUhRCzMaqkPtjKc+8GvpNSLgnjdZVOoGtByr/+N+4VX+AcNIqM39+K2a5mEilKTxZOQjgSGAS8BDyNUe7ipTCOywYKm90uBEbv/iQhRCJwLUbRvH2yZs2afT2k0+Tm5kY7hHbtNUa/h7hVc7CVbcXTbwyV/cdTsHpd5wZH1/8MVXwHRsV3YCIRXzgJoVBKGRBCbASGSyk/CJ3E22Om5c5qJkBr5XmXAXOklPtcTnv48OE4HI59PSzicnNzGTlyZLTDaNPeYvRXlVA0awb+il2kTbyehKN+F4Xouv5nqOI7MCq+A7O/8Xm93ja/SIeTEGpDg8OrgD8LITZgjCe0Jx8Y1+x2JrCrlef9HpgRxuspEebJlxR98ChoQbIuvpfYQ0dEOyRFUTpRODOJbsToNvoa4xv+d8ATYRz3DXCKECI9NGh8AfBl8ycIIUwYg84/7EvQSserXbOYwrcfwOxwkn3lDJUMFOUgFE4Lob+U8o7Q9YvCfWEpZYEQ4h7gW8AOvCqlXC6E+By4X0q5AmOqqU9K6dnXwJWOoes6lYtnUbV4FjF9h9HrgjuwOOOjHZaiKFEQTkKYJoR4GXgdeE1K2Vq3T6uklO8A7+x238Rm10swupKUKNACPko/e5G6tUuIG3Ey6Wder3Y3U5SDWLtdRlLKscBEjHGDH4UQnwkhzot4ZEpEmby1FL79AHVrl5Ay/lLSz75JJQNFOciFtRpZSrleSnkncD6QBrwX0aiUiPKV7CD+hzfxFW8n44LbSDrufFWTSFGUsFYqZ2BMDb0y9PzXMFYuK92MFvDhXvEFlYs/wGQyqzIUiqK0EM4YwibgI+BGtZK4e9J1jbq1S6lY+A6B6hJi+x9FYZ/jVTJQFKWFcBJCHymlO+KRKBHRkLeWivlv4i3cgr3XoWSedT/OQ49gVxdfhakoSucLZwtNlQy6IV9ZPhUL3qJ+0wos8amkn3MzcYefiMl0wEVsFUXpocJpISjdSKC2kspFs6hZ+Q0mewwp4y8lYdRZmG1dr8SHoihdi0oIPYTm81D941yqfvgEPegnYeQZJJ8wCYsrnLJTiqIo4c0y6gWMkVLOFUI8BhwD/F1KuSri0Snt0rUgNau+pXLRewRrK3GKMaROuAxbSna0Q1MUpZsJp4XwBvCVEGICcAbwDPB/wEkRjEtph67rNGz5hfIFb+Ev3YEjZzC9zr+NmD5Doh2aoijdVDgJIVVK+YwQ4gngHSnlG0KIGyMdmLJ33qKtVMz/Dw3bV2NNziTj/NtwDRmrFpcpinJAwkkIdiGEDTgTuDJUuTSc8tdKB/NXl1C58F1q1yzGHBtH6mnXkHD0aZgsquSEoigHLpyE8AlQCqyUUuYKIdawW8E6peMFgxq+QBCfX8NfV0PDT3Pwr/4a3WTCK35Hw6DTqHbGYdvhxmY1Y7WYmy7NZhMmE5hNJsxmE5bGS4sJq8WMxWzG6XRG+1dUFKWLCWcdwgNCiH8BBaG7pkgpf41sWD2brut4fEH8/iD+oE4gqOH3B6n3BoyfBh91DQECPi8JO5aQvnM+lqCX0tSjyM85BZ89CYq8gLfV1zeZwGI2YzEbicBsNjVdt4R+vPVBbPFVJDjtxNgtuGJtWCxqjYKiHMzCnWV0tJQyv3GWkRDibyop7JtgUMNd58Nd56OgtJaqWi/+gIY/EMQf0NCbbzaqa6RV/Eq/gq9x+KqoTBjMzt6nU+8Mr1K4rkMgqBEI7v05u3aVUO21YjGbiHVYccbaSE+KJTkhBleMlViH8aPGJRTl4LG/s4yeR80y2qvGk6jXH6SmzkeF22MkgRovDd5Am8cmVm+mb/6XuBoKqXVms6Xf+bgTBkQs1qCmU9vgp7bBT0lFPQB2mxmnw0a8y056UixJ8Q5i7BacMTbsNkvEYlEUJbrULKMOVtvgw+5KZfXmUgrL6qiu8+EPaO0e56zfRd/8/5Lk3ozHnsymQydTnnI4RKHUhM+v4fN7qar1srO4BpOJphZDSmIMaYmxOB3W0FiFCbMJTLuNVZjNZqxmk+qGUpRuRM0yOkCaplNT76Om3kdhWR0llQ1s2pJHRq/wunfs3kr67PqGtPJVBCwxbO89keKMMejmrrOIXNeh3hOg3hOgvNrDJqqAZoPWu49VmH67bbWYsVpM2G0WbBYzNpsZm8WMK9aGK9aGM8ZKrKPzZkkFNZ16jx+rxUyso+t8xorSFahZRvshGNSoqvVRXeuloLSGCreX+gY/jcMAgWD7LQJLoIGcwoVkliwDYFfmOHZlnkjQGhvByDuWputoQb3NsYq22G1mXDE2khNiSE+OJcFlJ9ZuxRVrw2brmCTR4PVT7wni8fqpcHspq26grsFPXKyNYf1TSU+KVeMkihIS9iwjKWV+6K6DepZRXYOfLQVVbN5ZhccX/pnQpPmx+9zY/W7ia3eSXbQIS9DTcuZQB9B1naCm4/UH8fmDeH3BUBdQsOm+QFBDD/iwxtSTnBCDzRqdbp3GrqnKGi9bC6qxWkw4Y2wkuOxYtBiKK+qJ3YcZUD5/kHqPH48vSHWtl7IqD9V1Xho8Abz+lv+v3KGxHXFICv2yEnDFqrUcihLOLCMzMEUIcSZgwxhgXielbHt0tIfRdZ3SqgZWby6jODT4ajygYQ3UY/e7m074ie4CUn160227z4012NDi9fZ15lAjry/A9sIaGryBFid541LD6w+iafpej7eE+vh9fo0tRTsBSHDZSU5wkJIQQ0pCDMnx0UkSgaDeNBNr165d7CzXccZYiXfaSE92kpIYg9NhtCDMJhP1ngAeX4Caej/l7gYqqz3UewM0eAMtZ23thccXZNWmUnaV1TK8fxoZKU4sZtVaUA5e4XQZPQIcATyHsQfztcATwN8iGFeX4vUHySt0s3ZrOUF3KQN2fUOMpwK7343NX4NZb/ntU8eE3xuHz5aAx5GCO64fPnsiPlsCPnsCXnsS3pjUfYrB5w+yIa8SmVdBIKhjNplw2M3YrRbsdgvxTjt2mwW7zYzDZsFus+x2acZus2ANfdPenpePzZlEpdtDhdtLSUUDeYU1Te+X4LIbySGUKKKRJDT9txlQheX1mIAYh5W4WBsWi4m6Bj/13gDBYBhn/zaUVjaw9NcCBvZOYkDvJOKd9o75BRSlmwknIZwBHCOl9AMIIeYBqzhIEkJVjYd12yrYUeTG3lDGYRtfxxpooNbVO3SiT2g60ftDl3mlNWTl9O6Q9/cHNDbuqGT99gr8AY0+veI5fEAqCS77AfV9221mstPjyEn/bX5AgzdAhdsTShIeiivq2V742/5ITUki3oHDbgkNGBuDxhaLMVhssZixWY3b5g7um9dDMbY3dXd/+Pwa67ZVUFRWx2ED0shKczUlT0U5WISTEMyNyQBASukVQvjbOqAnCAY1Ckpr+XVzGe46H7ENxQzd+DomXWPtkD9T79x7eWndVNch778pv4p12yrw+oJkp7kYMTCN5ISYA37tvYl1WMnZS5JoTBS7J4m2WELTTq0WU7PkYZTYSIxrbIEYC+G6ysBuRY2XH1YXcmhOAoP7JJMY1/EbC3m8AYKarsYtlC4nnISwUgjxDPACxpe0m4AePahcW+9j084qNu2sIhDUcNYVMHTTG+gmC2vFn/HEZkTsvYOazraCatZsLafBG6BXipMRA9NIS4rO7KPWkoTXF8QXCIZWQxulNwIBjUBQIxjU8Qc1gkEtdNn8ceN6bb2PXWW1Tf38dpuFlGZjGCkJMejhDAJESCCosWlHFcXl9QwfkEpOehw26/4vyPMHgtTWG11fxRX1lFbVo2mQleokK91FgtOhkoPSJYSTEG7E2P/ge8AE/Be4OZJBRYuu6xRX1LN6SxmllcYgcFztDoZsepOgJYZ1g6/Z577/cGm6Tl6hm9Vbyqlr8JOWFMOxh2fRK6XrFaFz2C047Ae2YtmYuuulwu1taoGs317RlCSsFhNpRTtbJAlnJ7ck3HU+flxTRN/MeMQhKaSE2ToLajp19T7qvAHKqxooKq+npt63R1dXda2XjTuqiHfZm5JDosuBM0YlByU6wkkId0spr4p0INHm8QXYvsvNum3lTdNJE9xbEZvfwmeLZ/3ga/A5OmZqaHO6rrOzuJbVW4yuqeR4B8cclUNWmqvLdKNEgsViJjUxltTE31o+vyUJD/lFlXh8wRZJonlLIivNRVpSbIePU+wuqOls2+WmpLKBww5NpU+vOBz2Pf/Z1HuMFkBVjTe0Qt1LXYO/3dlOmq5TXes1ksNOo9igkRziSHDZVXJQOlU4CeFs4O5IBxJNFW4Pa7aUUVD6WzdGUrVk8OZ38DhSWD/4avz2hA59T13X2VVWx6+by6iq8ZLgsnPCEdn0zoiLeCKw28w4Y2yYgOh1zOypeZJwWRrIzs5ukSQaWxPrt1ewblsFDpuFnAyjOysz1RnRQeC6Bj8r1hexqyyOoYem4IpLoLy6gZp6H0Xl9ZRXe6hr8BE4gBlPmqZTVWuUDJE7q0hw2clKdZGV5lLJQekU4SSErUKIr4AlQG3jnVLKpyMWVScJBIPkFxsDx7UNv42TJ1euZdDW92mIzWD9oKsJ2FxNj+m6Tnm1h6Cm03jabjx/N57I3fVBHNUNgOm3x0L/MWGi3uNnzdZyyqs9xMXaGDs8k0OyEiLybdduMxPrsBEXayMtKYakOAcxDivFRTaSUzNw1/uocnuoqvXi8QWbBjy7itZaEv6ARmFZHfklNewsrmFrQTUWs4msNBe9M+LITos74C6t1mg65JfUGif/mipiC9hjwVuHvZemU1XjparGy8YdlcS77GSnuchMdZHgcuCMafufrsPR8YPhSs8XTkKoCF0e2uy+rnPG2E/uOh8bd1SyJb+qxQkwtXwVA7fNptbVmw2DrmhRSqLS7eGn9cWUV3vaf4OtO9p82OmwMmpYL/pnJ2LuoMVQezv5t1bKevvmCoaJQ8nCSHY+f5AGr7Git74hQFWt8Y28weOnwRfA52+/HEdnsVnN9M2Mp29mPEFNp6SynoKSWvJDPyYTpCc56Z0RR05GHHEdPGDb4A1QVFZDtj2+Q193b4LNkoPMqyTBZW+admw2Eyow2KzQoMlEuduMtqkUk9mECVo814SJWIeFOKeNeKf9gAbMlZ4lnNIVVzdeF0I4pJSt78rSjQSDGr/IEgpKa1vcn176E/3zPsEd3w858HI0i/Etyx8I8uvmcjbtqMRuszBqWK+mxUuNs2H00H90oLy8nJSUlNDjxgPN+5LNJhOZqc4DqgS6Lyf/8F7P8ltp62SABIKaToPXj9cXxOML4q7zUun24q7z4QsYK6J13fg2q+k6uq6jaUa/eGexmE1Gt0qqi5FDMqhweykoqSG/pJafZQk/yxKS4h30zoijd0YcSXGObj02E9R0KmuMch9t2bWriOyatv++YuwWElwOMlOdpCXF4oo1/p668+fTHk3TqawxWsTxsXZiHBZcMWpzqEZ7TQhCCDvwL2COlPLj0N0fCiFKgT9399IVvt2a+pnF39Nv5zwqEwazceAUdLMtNOBbw8+ylAZvgIG9EzliUHr7ewL4qslOj0xBWLMJBvdN4ZCs+IhvYmMxm4iLtRMXaiQ1Tj31+gJ4/ZqRAEI/umYkR10HHR1NM2oqBYPGpaYZU06DmjH91OMNUF3nw+sLGLvHhVEivD0mk4nUxBhSE2MYMSidmnof+SW1FJTUsmZLOWu2lOOKsZITSgyuWBuuGBvOWCsWc+edEDRN77BW4YHw+IJ4fPWUVNZjNpmIc9pIjjcG7BPj7MQ57Th6yP4XmqZT4fawvcjNjkI3Hl8Qs9lklEaJtZOeHEtKQgyxodIosbHdp8hkR2qrhfAgkAAsbXbfdcCLwFTg3siF1bmyC7+jb8FXVCQNY1P/i9DNVmrqfaxYX0xReT3J8Q7GHZndoh87GixmE8P6pyL6Jkd1oxqH3YqjA6o7BIJBGjxGHSavP0htvY/qUBXZhlondpv5gLqq4p12hvZLYWi/FDzeAAWltRSU1rI5v3qPek+xDmMDoMYk4Yq1hi6N29Z2ynbouo7XF6TBF8DjNbrfPI3XfQE83gAN3iAeXwB/QDMGjNOMAeOM5NhOTUit0fTf6kjlFbmx28wkOO1kpDjJSHYarQenvdvVegpqOpVuD9t3VbOjuKZFQUpN0431IfV+CsvrmkqjOGOs4NdxFbqblWjvOosnI6mthHA2MEpK2VSVTUpZIIS4AviBnpAQdJ3eu76hd+FCylKOYPOhFxDUTKzbXMa67RVYzCZGDslgYJ+kiE9vbI/VYmbEwDQG9knEaukZ39qsFgvxLgu798QHghobEwNkZGbj8wepbfA3Tc00vtXu+5hGjMPKgFCtIk3TqfcGqGvwG/WQQlNG6z0BKqo95BfXsPu4usNmwRVrxRljnCDcbg+bivJ/O/H7gq1OMbVajC1KY+xWkuIdxDqc2KwWyqsb2LSjCplXicVsoleKk+xQgojrArWUfH6NsmoPZdXGrC5njI2kOAdZaa5Q6RIrjX2kjb92Y/do038bH9N1dBOYdKPOF+jExqUQDGoR66oJajoV7ga273Kzo7gGbxiViZuXRtm1q5jyegs2q1GiPTHOTnqyk8Q4xz5V4O1u2koIvubJoJGU0i2E6PbjCOg6h+z8nKyS7ylOO4Zth5xHYXkDKzYUU1vv55DMeI4SGV1iExW7zcxRgzPol5XQI/8Id2e1mGmoqyItqeXWocGg1jTwXVxRz9qt5fvV1WQ2m4gL9Ze3RtONLq06z55Jw11nTDM1m3TinMYmO8kJMcTaLU1jODGh6zF2a5sFAQMBjeLKegrL6tgV+gGId9rISosju7H1EOX/57pO0+dQUFqL1WImxmFpdsI3ugmbP9+4bOW+0PPKy0oprrGSleYiJTGGuFhbh0yrDQY1Ktwetu1ys7O45oBngfkDWtNU4LyimqZupkSXg97pcSQlOEhw9ZyB+bbOdkEhRLyUsqb5nUKIeIwy2O0SQkzBaEnYgGellC/u9rgA/okxjFkEXCylrNyH+PeLrmukbZhNYskPFGYcy/r00/nl1yJ2FNcQ77QxfmRvMlNd7b9QJ4ixWzh6SC/69orvEv3O0WSxmIlz2okDkuJjcMbY+EWWdHixO7PJFGoJ2EjfS8mQXbt2kZ2993pW4bBazU1lQXRdp6beT2FZHYVldWzJr2LjDqP1kNGs9dAVKrEa5UcObMynps5LYXkdheV1WCwm4mPtpCXHkpnqIj7WRrzLtk8t4cZEsLWgmvyS2ohOB27sZioorcVhs5AQ5yAn3VgoGe+0d8qXyEiNcbQV+bvAq0KIa6SUdQBCCBfwKvBhey8shMgBHgZGAl7geyHEt1LKdaHHTcBc4FYp5ZdCiEeBu4A7D+QXao+uBSn/7EUSC34gv9eJzA+O4tfvt6PpOocPSGXooSlR789t5IyxcszQTHLSe/aq5f1hMZvol5VAjN3CzxtKqKrt3o1Wk8nUNJ1UHJJMIKhRUlFvnDTL6sjdYLQe4pw2slJdpCbGkBhnfDvt7lVZg8HfFuRt3llFrMNKQpydrBQXqUkxodlPrSfCYFCjvCkR1HT69GivgJTK7gAAFCtJREFUP0hpZT2llfVYLebQYkInvVJdRiv0ABO4rut4fIHQ/ida0zhPrKXzE8KzwMtAkRBiLcZeCEOBmRgDzu05FVggpawAEELMBiY1O/ZooE5K+WXo9gyg42tD7CZQVUzd2kVszzqFf+/oT1VNqTFlcWhGl/j21SjeaWfUsF5dpqXSVWWmujh2RBarNpY2dbn0BFaLUZ68cbZa457dhWV1bC2oZtPOqqbnumJtJLrsJMY5SIwzLrvS4sJ91diPX1xeb8x0c9pISwy1HlzG2gmzyUR5tYct+VUUlNV2iXUygVArxVhNX2ls7BRq3cU77UbcbbTyg0HN2PTJb0xGqA6t0K9t8OPxBvD+f3vnHhznVR3w37fvh3alXb0lW46d2DcxeRCc0JaGlJbSmTK0JaVPGCClITCQodOmUMqj5THTUiiBaYF0Jo/CkAFaYOgrgYYG2tICaaNJAiHxSeo4xo5tyZZkabXvV/+4365Xq9Vbu5Ls85vZ0e797vft0d373fPdc889p1Cmio0ldvlIe2YhS15VRCrArcaY2lN+BXhYRE6t8tojQGPdU8CLGz5fhlU29wDXAk/RgaB5np5hHtz3Tu5/ZIJwsNyxcBFrIRELcv3BoS2LcLrTSMRCXH9wiB8dneLZE+cWLQhfCMQiAWJjAQ6MJazZInveI2s2bf+enkov+N+jR59dpCh22oyiXKm6/2eBI8/P1vdOBPweJqYzm+Ku3A4q1ar9XdIFjhw/RzTsJ9kdYteAjVEVDvrIuelt57MFZubynEvlXGVYXlVe9nawmo1px4Bj67i2h4U7mh2sUmn87pcBN4rII8aYDwN3ADev9gueeOKJNQs1PV/iocfPcPnuLnq7wFtOcepUauUT18jJkyfXdd7oQDeX9Id4/tjTHDvS3q0e4+Pjbb3+RlmrfPGuOKNJL48ePrlon0k7WO9vvFl4gWTYvugLUqkGyBUqpHMVMvkK6XyF2VSWU1PpBR5QIb9No2p3MNsyxw2rcv49NtSKW76wnv1rd0e3ek/DrunWx6JBz5a330q0S75oOEA0HCCTK5LJFZZNeduKSMjP5SO72nL/tnP14wTw0obPQ0BjC58GnhGRR9zPXwS+spYvuPLKK9cVs+XnbqjwrUeOc+bcIieqVRHw22xguSVc2da74DjSF+VFlw8Qj7Y/Ds34+DiHDh1q+/esl/XKt79cYdfIAI89faYtmdVqnD51iqHh4bZdf6M09sHmGcVc2gbhq1atz099Q2F9Y+HCsgpQrdQ+145VqVQrlN0NiRV38+FahrauiJ/BZITBRISBZKTti7G1hftzqRyRkH1iX8qdfDOcBlYiHrIbvdZKLU7Xeu6PfD6/7IN0O3+BfwM+YIzpB9LAa7D5mGt8F+g3xlwjIo8DvwRs60dWv8/DroEYl+3qxuvxcG4+z4nJFNNzOTK5jQ0+e4biXLO/b1v4oO9kvF4Pe0e6CQd9jD81wWy6sHnX9jgk4yEuGYlT3BehpydJoVghXyqTL5TJu4t/+WK5nhioniCoUtlw7uf14vE4xKNB4tEguwfbG3/J7lp3d7BXqlRqoU0aykrlCkePT5Ir+/nx6RRHTswCNkXrYDLCoLsZbiMBCmuDf82mbzP+5ReYYgJ+rw017gYN3A4u5ltN21rA3cT2XuDbQAC4W0T+xxjzAPAnrpnoJuAu13vpBPD6dsmzEXxe6x542e5u+rrP+4Unu0PsGY6TStunrucn5zk7m13TeoTjwKWj3Vx5aZ+GN95EhnqjvOTqER59ZpLTZzMbulY05Ge4L8rYUKy+KWt8/FkO7h9rWb9cqVIqlesZ48rlSj18R7FcqduNJ2eypLNFsrniBbPu4XEc8DqsNJQXswFGRkbqsYUmpjNMTGcWLJj3xIINCiK8pK9/8+A/44ZKrw3+Xo9DTyzI3pE4yXiInliQVLrAybPW7fXYaWsyTsaD9XDjW5mxbytpq0oUkS8AX2gqe2XD+4dZuNC8rfB6beC0/WMJ+ntCLf2ibWcL0RMLsXswRipTYKzPS6EaYnImQypdXDLYm8cBsyfJFXuThFokXVE2RiIe4sVXDPHk0SmOPD+7YrKaRrweh0Q8xN6ROP09Ebq7AqtW9F6PgzfgYyXDX7lcqXuQzMznmZzOkEoXSOdKW7ao2Gk8Hqce3vzg3l67w3g2y8RMlompDM8ct7u5HQeSsRADvREGesIUShV34M8xk8rXF5ebB/9EPEh3NLjIuycZtw9z1aoNFljbHPjk0Wl+dHQanwdGzp6shxe5WGYPF8d/uUZqoQQOjCXoX+bJpBnrSx6kkpvh+muvJZ0tkMoUmZhKc3IqTSp9PoHKdolLdKHTFQlwzf5+IiE/Tx6dXnGgjYR8jPR1LZgNtAuv1+N6AAUZ7I1ixhJ2xpAvkcoUOTOTZTqVJZMrrSr0woWA1+PQn4jQn4hw5b5eymUbQqM2gzj83DRPuYrd43FIxIJcMhwnEQ+RXGLwXw7HceopWl+wr5dCsczpqQxHfjzJmXMZfjxhZw89MRu2Y6TXzdTXpk2itbWb8ya3aoPJzZrf/H4PlaH2uKOrQmjA4zgMJMMcGEswkIise6CuVOygU/MmGOqNciBfdG/yDCfPpNk9GLug4hJtZ4IBH5dfkiQS8vHY02cWOQNsZDawmTiOY3diRwL0J2DfaHc9hEY2X+TsuSxnZrKkc1ZpXAxWDa/XUzcbgQ0lMTWbJRjwrnnwXw0Bv5exoRi+Sorh4WHOzZ+fPRx+bpqnjk7j93k2OKNvGvTrA//qQ8eXCsNcdcW+DcjQGlUIUE+ocmBPD4OJSFueCsNBP+Ggn4FEhP27E9assIP8wXc6vvpis5/xwxPMpQtEQjaWzp7BOIl4e2cD6yUU9BEK+oAQuwZilMpl5jM2EN/Js/NMTNs0nmt1XdwoHoctWffw+zwd26zpOA6JWIhELMTBvXb2MDGd4fRUmsIG9z/U3HG9Hjexkcd91d47NH0+/zcU9PJTB7s36b9cyPa7AzqI40Bvt50RDCUj7o3XftREtDU4jk2z+ZKrR5iezdHXEyYeXX736HbD5/XSE/PSE4OR/i7mswXmM0VOuyEu5jKFtngz+X0eusJ+kt1hBpMRIiFf3WOo5HpSFUv2VSqVKZQqFIo2z0W5UqVcqdgn4Ir9vBPXzAJ+L7sHY2331FqJYMC7bNDEjbDzfpVNIhbxc9muHgZ7I+rdc5FRsxlfCNgERtYsafYUSaWLTM5koJTG7/Oseyevz2sVQCIeYjAZIR4NEA37CAfXdq9Uq7VESVZx1Mwkzw/76OruZWo2y9mLzAy2nbkoFYLjOFx1WR/RJQJmKcpOpG6WTEbwlmboGxhlajbLick0s/P5ZSOAej0O0bCfRMwucHdHA25ymI09LDmOg8/r4PN6Fnhd/V9qioMHLmFsMEaxVCadLZIrlJlJ5Zh0zWCZXLHuhKF0hotSIXg8jioD5YImMz9Hv9lPfyLCpbt6mEsXmJ7LcWJynpm5HIVSpZ74Zag3Sk8sSDToI7IFOZX9PmsGA7t/xIxVyeQWelvNpHJkcsUlowMom8NFqRAU5WLC7/PWff33jnTb0BWlSj1/8HZbQ/F4Fntb5Qsl0jm7mD41a/cf1NYo7DpGhVKpcsFs8NsqVCEoykWEz+vZkesnNo+3j2Qcdg3EqFarFEsVCqUypZJdtC5XbFneTbOac8NoZ/MliuUKpVJ1gfJQ3bEYVQiKouw4HMch4PeuymOvFkqk4M4myuUq+UKZ58/O293hmULHFrN9XodY1OZGyORKFIrl+ms7zG5UISiKckFTDyXStGw4OtBFys1AduJMisnpLPPZzVcOAb+H7q4Qo31R+hJhYhG7+F8qV8jlSxRKZYolm8d7PlsklSkwny6QL5bJF23OhNVuWNsoqhAURbloiUUDxKIBRvqjpDJF5tJ5TkzOk5mfwYF1m5XCQR+JeJDR/i6S8RCxSGDRbMbn5ghvRblcIVewCqFUqpArlkhnrVuxDeteXKdky6MKQVGUi57GnNaj/V3EA1niPf0cn5jnzLkM6UxxWeXgODaVaW88zOhAF93utdYbjcDr9RANe4iGF7v9litVDh8+vK7rroQqBEVRlAYcxyGbmuHggX2M9ncx56bCPDGR4sw5G7IcrDdULGLD0Yz0ddEV8a+YN3kz8HocctmNhXRfClUIiqIoS+A4Tj0i7a7+LlKZ83ms+3vCdEX8dF1Ae5pUISiKoqwCj+e8crhQ0XCbiqIoCqAKQVEURXFRhaAoiqIAqhAURVEUF1UIiqIoCqAKQVEURXFRhaAoiqIAO3cfghegUChstRxLks/nt1qEFdnuMqp8G0Pl2xgXonwNY2bLMLFOdQcmMR0fH78B+M5Wy6EoirJDeemhQ4f+q7lwp84Q/hd4KXAK0Jx6iqIoq8MLDGPH0EXsyBmCoiiKsvnoorKiKIoCqEJQFEVRXFQhKIqiKIAqBEVRFMVFFYKiKIoCqEJQFEVRXFQhKIqiKMDO3Zi2LTDG/CnwG+7H+0XkXS2OvwmYcYvuEpFPd1C+bwMDQNEteouIPNxw/OeBO4Aw8Hci8r4OynYLcFtD0V7g8yJyW0OdLWk/Y0wc+C7wKhF5bjXtZIwZA+7DtrcArxOR+Q7JdyvwDqAKPIL9nQtN57wR+Agw4RbdLyLv7ZB8fwvcAKTdKh8Uka81nfNC4G4gDvwn8FYRKbVbPuAg8GcNh0eBh0XkVU3ndKT9Wo0pnex/qhDWifsj/QJwLfZG/IYx5qamjn4d8Fsi8r0tkM8BDgB7Wt1YxpgwcC/wM8Bx4H5jzC+KyNc7IZ+I3I0dADDGvAD4B+ADTdU63n7GmJ8A7sK23Vra6TPAZ0TkS8aY9wPvB/6oA/IdAN4JHAJSwGeBtwOfaDr1OuAPROSLmy3TcvI1fPeNInJqmVPvA24Rke8bY+4B3gzc2W75ROQB4AH32BDw38Dvtzi17e23xJjy28Bf0KH+pyaj9XMKuF1ECiJSBJ4CxprqXAe8xxjzA2PMp4wxoQ7KZ9y/DxpjHjfG3NZ0/MXAMyJy1FUY9wG/3kH5GrkTeI+InG0q34r2ezN2QD3pfl6xnYwxfuBG4Ctu0Web67RRvjzwNhGZE5Eq8EMW90OA64E3GmN+aIy5zxiT6IR8xpiIK8+97u/4QWPMgnHHGLMHCIvI992iz9K59mvkY8DfiMgzLY51ov1ajSkH6GD/U4WwTkTkR7UObIzZj53mPVA7bozpAh7FPr29COjBau1OkQAeAm4CXg681RjziobjI9gOWOMUsKtz4lncp6KwiHy5qXxL2k9EbhGRxsCJq2mnPmCuYSbWtrZslk9EjonINwGMMf1YM9w/tjj1FPBh4Grsk+anOiEfMAR8C2v6+0lsDLLfbTqtY32xhXxA/R5+GfBXS5za9vZbYkyp0MH+pyajDeKaO+4H3tn4ZOHa717ZUO/jWNNDW+y2zbhmlrqpxZ2GvxL4plvkwU5LazjYztdp3oK1jy5gq9uvgdW0U3MdWtRpK8aYUeDrwD0i8u/Nx0Xkpoa6HwWOdEIuEXkW+1BS++6/Bt6ANdvU2A598VasyaVlTOlOtl/jmAKUWGh+a2v/0xnCBjDG/DT2KfzdIvK5pmNjxpg3NRQ5nF/c7YRsNxhjXr7M95/ARj2sMUTraXTbMMYEsLbRf2pxbEvbr4HVtNMk0G2MqcWYH25Rp20YYy7HLpJ+TkQ+3OJ4tzGm0S7uYAeaTsh2lTHmNU3f3fw7bnlfBF4NfKnVgU62X4sxpaP9TxXCOjHG7MYuhL5WRFp1pCzwUWPMXneB9+3A11rUaxc9wMeMMSFjTAx4Y9P3PwwYY8xlbkd6LfYJs5NcDTwtIukWx7a6/Wqs2E6uvfc7wG+6RW9ortMu3N/2QeB9IvLxJarNA+9yF1TBmpU61ZYO8EljTMK1dd/a/N0icgzIuYMhwOvpYF80xvRhzZZHl6jSkfZbYkzpaP9ThbB+/hAIAXcYYx5zX281xjxgjLlORM5gzSH/jHUDc4ClbthNR0T+BTvtfBQYB+4Vke+5co6ISA64Gfgq8CRwmPOLUp1iH/YJqM52ab8ay7WTMeZuY8wvu1XfBtxqjHkSayfvlAvvLcAgcHtDP/xQo3wiUsbao+80xjyF9Uh619KX3DxE5AfAn2O9d54EHqt56tR+a7fq64BPGGMOA10sbctvB4v6oStfp9tv0ZiC7Xs306H+p/kQFEVRFEBnCIqiKIqLKgRFURQFUIWgKIqiuKhCUBRFUQBVCIqiKIqL7lRWlCUwxlyC3ZH6Q7fIg/VJ/6SI/P0Grvsg1tf8rDHmOeDXROSRjUmrKBtHFYKiLE9WRF5Y++AGYnvIGFMWka+u85qvWLmKonQe3YegKEvgzhCeEJGupvLXYvMP3Mj50MRe7CbAd4jInPvk/0Xs4N8DfFxE7jQ2N8DNwBPYWE3fwcaXuhYby/7z7cpToCgroWsIirJ2HgeuAt6NjWlzSESuwcaP+UhDvSQ2bPLLgA8ZY64Skd9xj/2siBx33+dE5DpsqO3b3RAGitJxVCEoytqpAhlsxq1fAR51wwy8GpuBq8anRaQqIieAb2CTn7TiCwAichqbkWugXYIrynLoGoKirJ3rsQvN3cDv1bJXuTkcGpP4NEbE9ADlJa7XGP2zio3bpCgdR2cIirIGjE1Z+X5soL1/BW4zxgTcLGB3YQO51XiDe84YdnZQi0BZBvwdE1pRVonOEBRlecKuOQhs0pEc8Mcicr8x5lvAX2IXk73AY8DtDefuNcaMY5Ojv0NExC3/MvAfxphf7ch/oCirRL2MFKUN6P4CZSeiJiNFURQF0BmCoiiK4qIzBEVRFAVQhaAoiqK4qEJQFEVRAFUIiqIoiosqBEVRFAVQhaAoiqK4/D840hsZ3aq6vQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### cs109Test(test_1.1b) ###\n",
    "# name the CV means and std variables cvmeans, cvstds and the train score train_scores  \n",
    "# your code here \n",
    "#fig, ax = plt.subplots(1, len(depths), figsize=(15, 5))\n",
    "lower = [i-2*j for (i,j) in zip(cvmeans,cvstds)]\n",
    "upper = [i+2*j for (i,j) in zip(cvmeans,cvstds)]\n",
    "plt.plot(depths,cvmeans,label = \"Cross validation performance\")\n",
    "plt.plot(depths,train_scores,label = \"Training performance\")\n",
    "plt.fill_between(depths, lower, upper,alpha=0.5)\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Cross validation performance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>Note:</b><span style = 'color:black'> Make sure your submission passes all assert statements we've provided in this notebook.</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.1 Check that you have the requested variables\n",
    "for var in ['train_scores', 'cvmeans', 'cvstds']:\n",
    "    assert var in globals(), f\"Variable '{var}' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** Select an appropriate depth and justify your choice using your cross-validation estimates. Then report the classification accuracy on the **test set**. Store the training and test accuracies in variables named `best_cv_tree_train_score` and `best_cv_tree_test_score` to refer to in a later question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most appropriate depth is 5 since it gives the best cross validation performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best cv tree has train score of 68.12%\n",
      "The best cv tree has test score of 64.82%\n"
     ]
    }
   ],
   "source": [
    "### cs109Test(test_1.3) ###\n",
    "# your code here\n",
    "dt = DecisionTreeClassifier(max_depth = 5)\n",
    "dt.fit(X_train,y_train)\n",
    "best_cv_tree_train_score = accuracy_score(dt.predict(X_train),y_train)\n",
    "best_cv_tree_test_score = accuracy_score(dt.predict(X_test),y_test)\n",
    "print('The best cv tree has train score of {}%'.format(best_cv_tree_train_score*100))\n",
    "print('The best cv tree has test score of {}%'.format(best_cv_tree_test_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.2 Check that you have the requested variables\n",
    "for var in ['best_cv_tree_train_score', 'best_cv_tree_test_score']:\n",
    "    assert var in globals(), f\"Variable '{var}' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3** What is the mechanism by which limiting the depth of the tree avoids over-fitting? What is one downside of limiting the tree depth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*\n",
    "\n",
    "Limiting the depth of the tree will prevent the decision from growing. The more the depth, the more binary decisions the tree can make and thus as depths increase, the model possibly could attain 100% training accuracy and thus overfit. \n",
    "One downside of limiting the tree depth is early stopping and miss out on good splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 2 [25 pts]: Bagging </b></div> \n",
    "Bagging is the technique of building the same model on multiple bootstrap samples from the data and combining each model's prediction to get an overall classification. In this question we build an example by hand and study how the number of bootstrapped datasets impacts the accuracy of the resulting classification.\n",
    "\n",
    "\n",
    "\n",
    "**2.1** Using decision trees, choose a tree depth that will overfit the training set. What evidence leads you to believe that this depth  overfits? Assign your choice to a variable named `tree_depth` here. (You may want to explore different settings for this value in the problems below.)\n",
    "\n",
    "**2.2** Create 55 bootstrapped replications of the original training data, and fit a decision tree to each. Use the tree depth you just chose in 2.1. Record each tree's prediction. In particular, produce a dataset like those shown (see below), where each row is a training and test observation, respectively, each column is one of the trees, and each entry is that tree's prediction for that observation. \n",
    "\n",
    "Store these results as `bagging_train_df` and `bagging_test_df`. Don't worry about visualizing these results yet.\n",
    "\n",
    "**2.3**  _Aggregate_ all 55 _bootstrapped_ models to get a combined prediction for each training and test point: predict a 1 if and only if a majority of the models predict that example to be from class 1. Assign the bagging accuracy test to a variable name `bagging_accuracy_test`. What accuracy does this *bagging* model achieve on the test set? Write an assertion that verifies that this test-set accuracy is at least as good as the accuracy for the model you fit in Question 1.\n",
    "\n",
    "**2.4** We want to know how the number of bootstraps affects our bagging ensemble's performance. Use the `running_predictions` function (given below) to get the model's accuracy score when using only 1,2,3,4,... of the bootstrapped models. Make a plot of training and test set accuracies as a function of number of bootstraps. \n",
    "** use the depth that you used above ** \n",
    "\n",
    "On your plot, also include horizontal lines for two baselines:\n",
    "- the test accuracy of the best model from question 1\n",
    "- the test accuracy of a single tree with the tree depth you chose in 2.1, trained on the full training set.\n",
    "\n",
    "**2.5** Referring to your graph from 2.4, compare the performance of bagging against the baseline of a single `tree_depth` tree. Explain the differences you see.\n",
    "\n",
    "**2.6** Bagging and limiting tree depth both affect how much the model overfits. Compare and contrast these two approaches. Your answer should refer to your graph in 2.4 and may duplicate something you said in your answer to 1.5.\n",
    "\n",
    "**2.7**: In what ways might our bagging classifier be overfitting the data? In what ways might it be underfitting?\n",
    "\n",
    "**Hints**\n",
    "- Use `resample` from sklearn to easily bootstrap the x and y data.\n",
    "- use `np.mean` to easily test for majority. If a majority of models vote 1, what does that imply about the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Using decision trees, choose a tree depth that will overfit the training set. What evidence leads you to believe that this depth  overfits? Assign your choice to a variable named `tree_depth` here. (You may want to explore different settings for this value in the problems below.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cs109Test (test_2.1)\n",
    "# Assign your choice to a variable named tree_depth\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1 Check that you have the requested variables\n",
    "assert 'tree_depth' in globals(), f\"Variable 'tree_depth' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Create 55 bootstrapped replications of the original training data, and fit a decision tree to each. Use the tree depth you just chose in 2.1. Record each tree's prediction. In particular, produce a dataset like those shown (see below), where each row is a training and test observation, respectively, each column is one of the trees, and each entry is that tree's prediction for that observation.\n",
    "\n",
    "Store these results as `bagging_train_df` and `bagging_test_df`. Don't worry about visualizing these results yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of `bagging_train_df` and `bagging_test_df`:**\n",
    "\n",
    "`bagging_train`:\n",
    "\n",
    "|     |bootstrap model 1's prediction|bootstrap model 2's prediction|...|bootstrap model 45's prediction|  \n",
    "| --- | --- | --- | --- | --- |\n",
    "|training row 1| binary value | binary value|... |binary value|\n",
    "|training row 2| binary value| binary value|... |binary value|\n",
    "|...| ...| ...|... |... |\n",
    "\n",
    "`bagging_test`:\n",
    "\n",
    "|     |bootstrap model 1's prediction|bootstrap model 2's prediction|...|bootstrap model 45's prediction|\n",
    "| --- | --- | --- | --- | --- |\n",
    "|test row 1| binary value | binary value|... |binary value|\n",
    "|test row 2| binary value| binary value|... |binary value|\n",
    "|...| ...| ...|... |... |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cs109test(test_2.2) ### \n",
    "\n",
    "def bagger(n_trees: int, tree_depth: int,  random_seed=0) -> (pd.DataFrame, pd.DataFrame, list):\n",
    "    \"\"\"A function that takes as \n",
    "    \n",
    "    Inputs:\n",
    "      n_tres\n",
    "      tree_depth \n",
    "      a random_seed (default =0)\n",
    "    \n",
    "    Returns:\n",
    "      bagging_train dataframe (as described above)\n",
    "      bagging_test dataframe (as described above)\n",
    "      bagging_models every trained model for each bootstrap (you will need this in Q3.2)\n",
    "    \"\"\"\n",
    "    # your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 55 \n",
    "bagging_train_df, bagging_test_df, bagging_models = bagger(n_trees, tree_depth,  random_seed= 0)\n",
    "display(bagging_train_df.head())\n",
    "display(bagging_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2 Check that you have the requested function & variables\n",
    "assert 'bagger' in globals(), f\"Function 'bagger()' does not exist!\"\n",
    "for var in ['bagging_train_df', 'bagging_test_df']:\n",
    "    assert var in globals(), f\"Variable '{var}' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3**  _Aggregate_ all 55 _bootstrapped_ models to get a combined prediction for each training and test point: predict a 1 if and only if a majority of the models predict that example to be from class 1. Assign the bagging accuracy test to a variable name `bagging_accuracy_test`. What accuracy does this *bagging* model achieve on the test set? Write an assertion that verifies that this test-set accuracy is at least as good as the accuracy for the model you fit in Question 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cs109test(test_2.3) ### \n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.3 Check that you have the requested variable\n",
    "assert 'bagging_accuracy_test' in globals(), f\"Variable 'bagging_accuracy_test' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** We want to know how the number of bootstraps affects our bagging ensemble's performance. Use the `running_predictions` function (given below) to get the model's accuracy score when using only 1,2,3,4,... of the bootstrapped models. Make a plot of training and test set accuracies as a function of number of bootstraps.\n",
    "** use the depth that you used above **\n",
    "\n",
    "On your plot, also include horizontal lines for two baselines:\n",
    "- the test accuracy of the best model from question 1\n",
    "- the test accuracy of a single tree with the tree depth you chose in 2.1, trained on the full training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_predictions(prediction_dataset: np.ndarray, targets: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"A function to predict examples' class via the majority among trees (ties are predicted as 0)\n",
    "    \n",
    "    Inputs:\n",
    "      prediction_dataset - a (n_examples by n_sub_models) dataset (not a dataframe), where each entry [i,j] is sub-model j's prediction\n",
    "          for example i\n",
    "      targets - the true class labels\n",
    "    \n",
    "    Returns:\n",
    "      a vector where vec[i] is the model's accuracy when using just the first i+1 sub-models\n",
    "    \"\"\"\n",
    "    \n",
    "    n_trees = prediction_dataset.shape[1]\n",
    "    \n",
    "    # find the running percentage of models voting 1 as more models are considered\n",
    "    running_percent_1s = np.cumsum(prediction_dataset, axis=1)/np.arange(1,n_trees+1)\n",
    "    \n",
    "    # predict 1 when the running average is above 0.5\n",
    "    running_conclusions = running_percent_1s > 0.5\n",
    "    \n",
    "    # check whether the running predictions match the targets\n",
    "    running_correctnesss = running_conclusions == targets.reshape(-1,1)\n",
    "    \n",
    "    return np.mean(running_correctnesss, axis=0)\n",
    "    # returns a 1-d series of the accuracy of using the first n trees to predict the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting code\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.4 Check that you have the requested function\n",
    "assert 'running_predictions' in globals(), f\"Function 'running_predictions()' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5** Referring to your graph from 2.4, compare the performance of bagging against the baseline of a single `tree_depth` tree. Explain the differences you see.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Bagging and limiting tree depth both affect how much the model overfits. Compare and contrast these two approaches. Your answer should refer to your graph in 2.4 and may duplicate something you said in your answer to 1.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7**: In what ways might our bagging classifier be overfitting the data? In what ways might it be underfitting?\n",
    "\n",
    "**Hints**\n",
    "- Use `resample` from sklearn to easily bootstrap the x and y data.\n",
    "- use `np.mean` to easily test for majority. If a majority of models vote 1, what does that imply about the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 3 [20 pts]: Random Forests </b> </div>\n",
    "Random Forests are closely related to the bagging model we built by hand in question 2. In this question we compare our by-hand results with the results of using `RandomForestClassifier` directly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**3.1**  Fit a `RandomForestClassifier` to the original `X_train` data using the same tree depth and number of trees that you used in Question 2.2. Use number of features to consider when looking for the best split to be the `sqrt(total_number of features`. Evaluate its accuracy on the test set and assign it to a variable name `random_forest_test_score`.\n",
    "\n",
    "**3.2** Among all of the decision trees you fit in the bagging process, how many times is each feature used as the top/first node? How about for each tree in the random forest you just fit? What about the process of training the Random Forest causes this difference? What implication does this observation have on the accuracy of bagging vs Random Forest?\n",
    "Assign this to two pandas Series called `top_predictors_bagging` and `top_predictors_rf` and give them an example] \n",
    "\n",
    "**Hint**: A decision tree's top feature is stored in `model.tree_.feature[0]`. A random forest object stores its decision trees in its `.estimators_` attribute.\n",
    "\n",
    "\n",
    "**3.3**: Make a pandas table of the training and test accuracy for the following models and name it `results_df`:\n",
    "\n",
    "- Single tree with best depth chosen by cross-validation (from Question 1)\n",
    "- A single overfit tree trained on all data (from Question 2, using the depth you chose there)\n",
    "- Bagging 55 such trees (from Question 2)\n",
    "- A Random Forest of 55 such trees (from Question 3.1)\n",
    "\n",
    "(see below for the expected structure)  \n",
    "\n",
    "(This problem should not require fitting any new models, though you may need to go back and store the accuracies from models you fit previously.)\n",
    "\n",
    "What is the relative performance of each model on the training set? On the test set? Comment on how these relationships make sense (or don't make sense) in light of how each model treats the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1**  Fit a `RandomForestClassifier` to the original `X_train` data using the same tree depth and number of trees that you used in Question 2.2. Use number of features to consider when looking for the best split to be the `sqrt(total_number of features`. Evaluate its accuracy on the test set and assign it to a variable name `random_forest_test_score`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cs109test(test_3.1) ### \n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1 Check that you have the requested variable\n",
    "assert 'random_forest_test_score' in globals(), f\"Variable 'random_forest_test_score' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Among all of the decision trees you fit in the bagging process, how many times is each feature used as the top/first node? How about for each tree in the random forest you just fit? What about the process of training the Random Forest causes this difference? What implication does this observation have on the accuracy of bagging vs Random Forest?\n",
    "Assign this to two pandas Series called `top_predictors_bagging` and `top_predictors_rf` and give them an example]\n",
    "\n",
    "**Hint**: A decision tree's top feature is stored in `model.tree_.feature[0]`. A random forest object stores its decision trees in its `.estimators_` attribute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</span></div><div class='alert alert-block alert-danger'><b>WARNING!:</b><span style = 'color:black'> Do not delete any of the `### cs109Test() ###` comment lines!</span></div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cs109test(test_3.2) ### \n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2 Check that you have the requested variables\n",
    "for var in ['top_predictors_bagging', 'top_predictors_rf']:\n",
    "    assert var in globals(), f\"Variable '{var}' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3**: Make a pandas table of the training and test accuracy for the following models and name it `results_df`:\n",
    "\n",
    "- Single tree with best depth chosen by cross-validation (from Question 1)\n",
    "- A single overfit tree trained on all data (from Question 2, using the depth you chose there)\n",
    "- Bagging 55 such trees (from Question 2)\n",
    "- A Random Forest of 55 such trees (from Question 3.1)\n",
    "\n",
    "(see below for the expected structure)\n",
    "\n",
    "(This problem should not require fitting any new models, though you may need to go back and store the accuracies from models you fit previously.)\n",
    "\n",
    "What is the relative performance of each model on the training set? On the test set? Comment on how these relationships make sense (or don't make sense) in light of how each model treats the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following table.\n",
    "\n",
    "\n",
    "| classifier | training accuracy | test accuracy |\n",
    "| --- | --- | --- |\n",
    "| single tree with best depth chosen by CV | | |\n",
    "| single depth-X tree | | |\n",
    "| bagging 55 depth-X trees | | |\n",
    "| Random Forest of 55 depth-X trees | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.3 Check that you have the requested variable\n",
    "assert 'results_df' in globals(), \"Variable 'results_df' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'><b> Question 4 [15 pts]: Boosting </div>\n",
    "In this question we explore a different kind of ensemble method, boosting, where each new model is trained on a dataset weighted towards observations that the current set of models predicts incorrectly. \n",
    "\n",
    "We'll focus on the AdaBoost flavor of boosting and examine what happens to the ensemble model's accuracy as the algorithm adds more estimators (iterations) to the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** We'll motivate AdaBoost by noticing patterns in the errors that a single classifier makes. Fit `tree1`, a decision tree with depth 3, to the training data. \n",
    "Report the train and test accuracies. For each predictor, make a plot that compares two distributions: the values of that predictor for examples that `tree1` classifies correctly, and the values of that predictor for examples that `tree1` classifies incorrectly. Do you notice any predictors for which the distributions are clearly different?\n",
    "\n",
    "*Hints*:\n",
    "- If you have `fig, axs = plt.subplots(...)`, then `axs.ravel()` gives a list of each plot in reading order.\n",
    "- [`sns.kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) takes `ax` and `label` parameters.\n",
    "\n",
    "**4.2** The following code (see below) attempts to implement a simplified version of boosting using just two classifiers (described below). However, it has both stylistic and functionality flaws. First, imagine that you are a grader for a Data Science class; write a comment for the student who submitted this code. Then, imagine that you're the TF writing the solutions; make an excellent example implementation. Finally, use your corrected code to compare the performance of `tree1` and the boosted algorithm on both the training and test set.\n",
    "\n",
    "**4.3** Now let's use the sklearn implementation of AdaBoost: Use `AdaBoostClassifier` to fit another ensemble to `X_train`. Use a decision tree of depth 3 as the base learner and a learning rate 0.05, and run the boosting for 800 iterations. Make a plot of the effect of the number of estimators/iterations on the model's train and test accuracy.\n",
    "\n",
    "*Hint*: The `staged_score` method provides the accuracy numbers you'll need. You'll need to use `list()` to convert the \"generator\" it returns into an ordinary list.\n",
    "\n",
    "**4.4** Repeat the plot above for a base learner with depth of (1, 2, 3, 4). What trends do you see in the training and test accuracy?\n",
    "\n",
    "(It's okay if your code re-fits the depth-3 classifier instead of reusing the results from the previous problem.)\n",
    "\n",
    "**4.5** Based on the plot you just made, what combination of base learner depth and number of iterations seems optimal? Why? How does the performance of this model compare with the performance of the ensembles you considered above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** We'll motivate AdaBoost by noticing patterns in the errors that a single classifier makes. Fit `tree1`, a decision tree with depth 3, to the training data.\n",
    "Report the train and test accuracies. For each predictor, make a plot that compares two distributions: the values of that predictor for examples that `tree1` classifies correctly, and the values of that predictor for examples that `tree1` classifies incorrectly. Do you notice any predictors for which the distributions are clearly different?\n",
    "\n",
    "*Hints*:\n",
    "- If you have `fig, axs = plt.subplots(...)`, then `axs.ravel()` gives a list of each plot in reading order.\n",
    "- [`sns.kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) takes `ax` and `label` parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1 Check that you have the requested variable\n",
    "assert 'tree1' in globals(), \"Variable 'tree1' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2** The following code (see below) attempts to implement a simplified version of boosting using just two classifiers (described below). However, it has both stylistic and functionality flaws. First, imagine that you are a grader for a Data Science class; write a comment for the student who submitted this code. Then, imagine that you're the TF writing the solutions; make an excellent example implementation. Finally, use your corrected code to compare the performance of `tree1` and the boosted algorithm on both the training and test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intended functionality is the following:\n",
    "1. Fit `tree1`, a decision tree with max depth 3.\n",
    "2. Construct an array of sample weights. Give a weight of 1 to samples that `tree1` classified correctly, and 2 to samples that `tree1` misclassified.\n",
    "3. Fit `tree2`, another depth-3 decision tree, using those sample weights.\n",
    "4. To predict, compute the probabilities that `tree1` and `tree2` each assign to the positive class. Take the average of those two probabilities as the prediction probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree1 accuracy: 0.3582\n",
      "Boosted accuracy: 0.0008\n",
      "Boosted accuracy: 0.002\n"
     ]
    }
   ],
   "source": [
    "def boostmeup(X, y):\n",
    "    tree = DecisionTreeClassifier(max_depth=3)\n",
    "    tree1 = tree.fit(X, y)\n",
    "    sample_weight = np.ones(len(X_train))\n",
    "    q = 0\n",
    "    for idx in range(len(X_train)):\n",
    "          if tree1.predict([X_train[idx]]) != y_train[idx]:\n",
    "             sample_weight[idx] = sample_weight[idx] * 2\n",
    "             q = q + 1\n",
    "    print(\"tree1 accuracy:\", q / len(X_train))\n",
    "    tree2 = tree.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    \n",
    "# Train\n",
    "    q = 0\n",
    "    for idx in range(len(X_train)):\n",
    "        t1p = tree1.predict_proba([X_train[idx]])[0][1]\n",
    "        t2p = tree2.predict_proba([X_train[idx]])[0][1]\n",
    "        m = (t1p + t2p) / 2\n",
    "        if m > .5:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 0\n",
    "            else:\n",
    "                q = q + 1\n",
    "        else:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 1\n",
    "            else:\n",
    "                q = 0\n",
    "    print(\"Boosted accuracy:\", q / len(X_train))\n",
    "\n",
    "# Test\n",
    "    q = 0\n",
    "    for idx in range(len(X_test)):\n",
    "        t1p = tree1.predict_proba([X_test[idx]])[0][1]\n",
    "        t2p = tree2.predict_proba([X_test[idx]])[0][1]\n",
    "        m = (t1p + t2p) / 2\n",
    "        if m > .5:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 0\n",
    "            else:\n",
    "                q = q + 1\n",
    "        else:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 1\n",
    "            else:\n",
    "                q = 0\n",
    "    print(\"Boosted accuracy:\", q / len(X_test))\n",
    "\n",
    "boostmeup(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.2 Check that you have the requested variables\n",
    "for var in ['tree1', 'tree2']:\n",
    "    assert var in globals(), f\"Variable '{var}' does not exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3** Now let's use the sklearn implementation of AdaBoost: Use `AdaBoostClassifier` to fit another ensemble to `X_train`. Use a decision tree of depth 3 as the base learner and a learning rate 0.05, and run the boosting for 800 iterations. Make a plot of the effect of the number of estimators/iterations on the model's train and test accuracy.\n",
    "\n",
    "*Hint*: The `staged_score` method provides the accuracy numbers you'll need. You'll need to use `list()` to convert the \"generator\" it returns into an ordinary list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4** Repeat the plot above for a base learner with depth of (1, 2, 3, 4). What trends do you see in the training and test accuracy?\n",
    "\n",
    "(It's okay if your code re-fits the depth-3 classifier instead of reusing the results from the previous problem.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.5** Based on the plot you just made, what combination of base learner depth and number of iterations seems optimal? Why? How does the performance of this model compare with the performance of the ensembles you considered above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 5 [15 pts]: Understanding </b></div>\n",
    "This question is an overall test of your knowledge of this homework's material. You may need to refer to lecture notes and other material outside this homework to answer these questions.\n",
    "\n",
    "\n",
    "\n",
    "**5.1** How do boosting and bagging relate: what is common to both, and what is unique to each?\n",
    "\n",
    "\n",
    "**5.2** Reflect on the overall performance of all of the different classifiers you have seen throughout this assignment. Which performed best? Why do you think that may have happened?\n",
    "\n",
    "**5.3** What is the impact of having too many trees in boosting and in bagging? In which instance is it worse to have too many trees?\n",
    "\n",
    "**5.4** Which technique, boosting or bagging, is better suited to parallelization, where you could have multiple computers working on a problem at the same time?\n",
    "\n",
    "**5.5** Which of these techniques can be extended to regression tasks? How?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1** How do boosting and bagging relate: what is common to both, and what is unique to each?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2** Reflect on the overall performance of all of the different classifiers you have seen throughout this assignment. Which performed best? Why do you think that may have happened?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3** What is the impact of having too many trees in boosting and in bagging? In which instance is it worse to have too many trees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4** Which technique, boosting or bagging, is better suited to parallelization, where you could have multiple computers working on a problem at the same time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5** Which of these techniques can be extended to regression tasks? How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
